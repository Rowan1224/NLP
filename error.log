ERROR:root:sen: The pattern [ /[b-g]/ ] specifies one of the characters b, c, d, e, f, or g. Some other examples are shown in Figure [ 2.3 ]., ans: ['Figure [ 2.3] ']
ERROR:root:sen: The square braces can also be used to specify what a single character cannot be, by use of the caret ˆ., ans: ['caret  ⁇  ']
ERROR:root:sen: If the caret ˆ is the first symbol after the open square brace [, the resulting pattern is negated., ans: ['the caret  ⁇  is the first symbol after the open square brace ']
ERROR:root:sen: baaa!, ans: ['baaaa! ']
ERROR:root:sen: baaaaa!, ans: ['baa! baaa! baaaa! ']
ERROR:root:sen: . . ., ans: ['sheep ']
ERROR:root:sen: The most common anchors are the caretˆand the dollar sign $., ans: ['caret ⁇ and the dollar sign $ ']
ERROR:root:sen: There are also two other anchors: \b matches a word boundary, and \B matches a non-boundary., ans: [' ⁇ b ']
ERROR:root:sen: The operator *?, ans: ['Kleene star ']
ERROR:root:sen: The operator +?, ans: ['Kleene plus ']
ERROR:root:sen: Suppose we wanted to do this without the use of [ /\b/ ]., ans: ['[ / ⁇ b/ ]. ']
ERROR:root:sen: The regular expression [ /{3}/ ] means "exactly 3 occurrences of the previous character or expression"., ans: ['exact 3 occurrences of the previous character or expression ']
ERROR:root:sen: So /a\., ans: ['/a ⁇  ']
ERROR:root:sen: REs for counting are summarized in Figure 2 .9., ans: ['Figure 2.9 ']
ERROR:root:sen: The most common of these are the newline character newline \n and the tab character \t., ans: ['newline character newline  ⁇ n and the tab character  ⁇ t ']
ERROR:root:sen: We’ll add a decimal point and two digits afterwards: /$[0-9]+\., ans: ['/$[0-9]+ ⁇  ']
ERROR:root:sen: optional and to make sure we’re at a word boundary: /(ˆ|\W)$[0-9]+(\., ans: ['/( ⁇ | ⁇ W)$[0-9]+( ⁇.] ']
ERROR:root:sen: One last catch!, ans: ['$199999.99 ']
ERROR:root:sen: )\b/, ans: ['GB ']
ERROR:root:sen: Here's how it looks: [ s[ /([0-9]+)/ ]<\1>/ ], ans: ['[ s[ /([0-9]+)/ ] ⁇ 1>/ ']
ERROR:root:sen: We do this by surrounding the first X with the parenthesis operator, and replacing the second X with the number operator \1, as follows: /the (., ans: [' ⁇ 1 ']
ERROR:root:sen: *)er they were, the \1er they will be/, ans: [' ⁇ 1er they will be ']
ERROR:root:sen: *)er they (., ans: ['they (. *)er they (. *) ']
ERROR:root:sen: Similarly, the third capture group is stored in \\3, the fourth is \\4, and so on., ans: [' ⁇ 4 ']
ERROR:root:sen: /(?, ans: ['/(?) ']
ERROR:root:sen: ELIZA 3 : YOUR BOYFRIEND MADE YOU COME HERE User 4 : He says I'm depressed much of the time., ans: ['DEPRESSED ']
ERROR:root:sen: Input lines are [ 2.2 ] \u2022 WORDS 11 first uppercased., ans: ['[ 2.2 ]  ⁇ u2022 WORDS 11 ']
ERROR:root:sen: The operator (?!, ans: ['pattern ']
ERROR:root:sen: What do you think they are?, ans: ['what do you think they are ']
ERROR:root:sen: Figure 2 .11 shows the rough numbers of types and tokens computed from some popular English corpora., ans: ['Figure 2.11 ']
ERROR:root:sen: [ 2.1 ], where k and β are positive constants, and 0 < β < 1., ans: ['k and  ⁇  ']
ERROR:root:sen: The value of β depends on the corpus size and the genre, but at least for the large corpora in Figure 2 .11, β ranges from .67 to .75., ans: ['.67 to.75 ']
ERROR:root:sen: How can a user of a dataset know all these details?, ans: ['how can a user of a dataset know all these details ']
ERROR:root:sen: Motivation: Why was the corpus collected, by whom, and who funded it?, ans: ['why ']
ERROR:root:sen: Situation: When and in what situation was the text written/spoken?, ans: ['when and in what situation ']
ERROR:root:sen: Normalizing word formats 3., ans: ['normalizing ']
ERROR:root:sen: The pattern [ /[b-g]/ ] specifies one of the characters b, c, d, e, f, or g. Some other examples are shown in Figure [ 2.3 ]., ans: ['Figure [ 2.3] ']
ERROR:root:sen: The square braces can also be used to specify what a single character cannot be, by use of the caret ˆ., ans: ['caret  ⁇  ']
ERROR:root:sen: If the caret ˆ is the first symbol after the open square brace [, the resulting pattern is negated., ans: ['the caret  ⁇  is the first symbol after the open square brace ']
ERROR:root:sen: baaa!, ans: ['baaaa! ']
ERROR:root:sen: baaaaa!, ans: ['baa! baaa! baaaa! ']
ERROR:root:sen: . . ., ans: ['sheep ']
ERROR:root:sen: The most common anchors are the caretˆand the dollar sign $., ans: ['caret ⁇ and the dollar sign $ ']
ERROR:root:sen: There are also two other anchors: \b matches a word boundary, and \B matches a non-boundary., ans: [' ⁇ b ']
ERROR:root:sen: The operator *?, ans: ['Kleene star ']
ERROR:root:sen: The operator +?, ans: ['Kleene plus ']
ERROR:root:sen: Suppose we wanted to do this without the use of [ /\b/ ]., ans: ['[ / ⁇ b/ ]. ']
ERROR:root:sen: The regular expression [ /{3}/ ] means "exactly 3 occurrences of the previous character or expression"., ans: ['exact 3 occurrences of the previous character or expression ']
ERROR:root:sen: So /a\., ans: ['/a ⁇  ']
ERROR:root:sen: REs for counting are summarized in Figure 2 .9., ans: ['Figure 2.9 ']
ERROR:root:sen: The most common of these are the newline character newline \n and the tab character \t., ans: ['newline character newline  ⁇ n and the tab character  ⁇ t ']
ERROR:root:sen: We’ll add a decimal point and two digits afterwards: /$[0-9]+\., ans: ['/$[0-9]+ ⁇  ']
ERROR:root:sen: optional and to make sure we’re at a word boundary: /(ˆ|\W)$[0-9]+(\., ans: ['/( ⁇ | ⁇ W)$[0-9]+( ⁇.] ']
ERROR:root:sen: One last catch!, ans: ['$199999.99 ']
ERROR:root:sen: )\b/, ans: ['GB ']
ERROR:root:sen: Here's how it looks: [ s[ /([0-9]+)/ ]<\1>/ ], ans: ['[ s[ /([0-9]+)/ ] ⁇ 1>/ ']
ERROR:root:sen: We do this by surrounding the first X with the parenthesis operator, and replacing the second X with the number operator \1, as follows: /the (., ans: [' ⁇ 1 ']
ERROR:root:sen: *)er they were, the \1er they will be/, ans: [' ⁇ 1er they will be ']
ERROR:root:sen: *)er they (., ans: ['they (. *)er they (. *) ']
ERROR:root:sen: Similarly, the third capture group is stored in \\3, the fourth is \\4, and so on., ans: [' ⁇ 4 ']
ERROR:root:sen: /(?, ans: ['/(?) ']
ERROR:root:sen: ELIZA 3 : YOUR BOYFRIEND MADE YOU COME HERE User 4 : He says I'm depressed much of the time., ans: ['DEPRESSED ']
ERROR:root:sen: Input lines are [ 2.2 ] \u2022 WORDS 11 first uppercased., ans: ['[ 2.2 ]  ⁇ u2022 WORDS 11 ']
ERROR:root:sen: The operator (?!, ans: ['pattern ']
ERROR:root:sen: What do you think they are?, ans: ['what do you think they are ']
ERROR:root:sen: Figure 2 .11 shows the rough numbers of types and tokens computed from some popular English corpora., ans: ['Figure 2.11 ']
ERROR:root:sen: [ 2.1 ], where k and β are positive constants, and 0 < β < 1., ans: ['k and  ⁇  ']
ERROR:root:sen: The value of β depends on the corpus size and the genre, but at least for the large corpora in Figure 2 .11, β ranges from .67 to .75., ans: ['.67 to.75 ']
ERROR:root:sen: How can a user of a dataset know all these details?, ans: ['how can a user of a dataset know all these details ']
ERROR:root:sen: Motivation: Why was the corpus collected, by whom, and who funded it?, ans: ['why ']
ERROR:root:sen: Situation: When and in what situation was the text written/spoken?, ans: ['when and in what situation ']
ERROR:root:sen: Normalizing word formats 3., ans: ['normalizing ']
ERROR:root:sen: The algorithm is based on series of rewrite rules run in series, as a cascade, in cascade which the output of each pass is fed as input to the next pass; here is a sampling of the rules: ATIONAL → ATE (e.g., relational → relate) ING → if stem contains vowel (e.g., motoring → motor) SSES → SS (e.g., grasses → grass), ans: ['SSES  ⁇  SS ', ' a cascade ']
ERROR:root:sen: Figure 2 .14 Representing the minimum edit distance between two strings as an alignment., ans: ['Figure 2.14 ']
ERROR:root:sen: Imagine some string (perhaps it is exention) that is in this optimal path (whatever it is)., ans: ['imagine some string ']
ERROR:root:sen: Why?, ans: ['shorter ']
ERROR:root:sen: The algorithm is summarized in Figure [ 2.17 ]; Figure [ 2.18 ] shows the results of applying the algorithm to the distance between intention and execution with the version of Levenshtein in Eq., ans: ['Figure [ 2.17] ', ' Levenshtein ']
ERROR:root:sen: Figure 2 .19 shows this path with the boldfaced cell., ans: ['Figure 2.19 ']
ERROR:root:sen: Figure 2 .19 also shows the intuition of how to compute this alignment path., ans: ['Figure 2.19 ']
ERROR:root:sen: While we worked our example with simple Levenshtein distance, the algorithm in Figure 2 .17 allows arbitrary weights on the operations., ans: ['Figure 2.17 ']
ERROR:root:sen: For the joint probability of each word in a sequence having a particular value P(X = w 1 ,Y = w 2 , Z = w 3 , ...,W = w n ) we'll use P(w 1 , w 2 , ..., w n )., ans: ['P(w 1, w 2,..., w n) ']
ERROR:root:sen: Now how can we compute probabilities of entire sequences like P(w 1 , w 2 , ..., w n )?, ans: ['P(w 1, w 2,..., w n) ']
ERROR:root:sen: We don't know any way to compute the exact probability of a word given a long sequence of preceding words, P(w n |w n−1 1 )., ans: ['P(w n |w n ⁇ 1 1 ']
ERROR:root:sen: Given the bigram assumption for the probability of an individual word, we can compute the probability of a complete word sequence by substituting Eq., ans: ['substituting Eq. [ 3.7] into Eq. [ 3.4] ']
ERROR:root:sen: We can simplify this equation, since the sum of all bigram counts that start with a given word w n−1 must be equal to the unigram count for that word w n−1 (the reader should take a moment to be convinced of this):, ans: ['the unigram count ', ' the sum of all bigram counts that start with a given word w n ⁇ 1 ']
ERROR:root:sen: We'll first need to augment each sentence with a special symbol <s> at the beginning of the sentence, to give us the bigram context of the first word., ans: [' ⁇ s> ']
ERROR:root:sen: The MLE of its probability is 400 1000000 or .0004., ans: ['400 1000000 or.0004 ']
ERROR:root:sen: Figure 3 .1 shows the bigram counts from a piece of a bigram grammar from the Berkeley Restaurant Project., ans: ['Figure 3.1 ', ' Berkeley Restaurant Project ']
ERROR:root:sen: For example, to compute trigram probabilities at the very beginning of the sentence, we use two pseudo-words for the first trigram (i.e., P(I|<s><s>)., ans: ['P(I| ⁇ s> ⁇ s> ']
ERROR:root:sen: Adding in log space is equivalent to multiplying in linear space, so we combine log probabilities by adding them., ans: ['adding log probabilities ']
ERROR:root:sen: [ 3.15 ] or Eq., ans: ['the entire sequence of words in some test set ']
ERROR:root:sen: Since this sequence will cross many sentence boundaries, we need to include the begin-and end-sentence markers <s> and </s> in the probability computation., ans: ['begin-and-end-sentence markers ']
ERROR:root:sen: By Eq., ans: ['Eq. 3.15 ']
ERROR:root:sen: Now we see the following test set: 0 0 0 0 0 3 0 0 0 0., ans: ['0 0 0 0 0 0 3 0 0 0. ']
ERROR:root:sen: Figure 3 .3 shows a visualization, using a unigram LM computed from the text of this book., ans: ['Figure 3.3 ']
ERROR:root:sen: Figure 3 .3 A visualization of the sampling distribution for sampling sentences by repeatedly sampling unigrams., ans: ['Figure 3.3 ']
ERROR:root:sen: We can use the same technique to generate bigrams by first generating a random bigram that starts with <s> (according to its bigram probability)., ans: [' ⁇ s> ']
ERROR:root:sen: To give an intuition for the increasing power of higher-order n-grams, Figure 3 .4 shows random sentences generated from unigram, bigram, trigram, and 4gram models trained on Shakespeare's works., ans: ['Figure 3.4 ']
ERROR:root:sen: Figure 3 .5 shows sentences generated by unigram, bigram, and trigram grammars trained on 40 million words from WSJ., ans: ['Figure 3.5 ']
ERROR:root:sen: Compare these examples to the pseudo-Shakespeare in Figure 3 .4., ans: ['Figure 3.4 ']
ERROR:root:sen: These zerosthings that don't ever occur in the training set but do occur in zeros the test set-are a problem for two reasons., ans: ['zeros things ']
ERROR:root:sen: is one in which we model these potential unknown words in the test set by adding a pseudo-word called <UNK>., ans: [' ⁇ UNK> ']
ERROR:root:sen: [ 1. ], ans: ['Choose a vocabulary (word list) that is fixed in advance ']
ERROR:root:sen: [ 2. ], ans: ['any word that is not in this set ']
ERROR:root:sen: Convert in the training set any word that is not in this set (any word) to the unknown word token <UNK> in a text normalization step., ans: [' ⁇ UNK> ']
ERROR:root:sen: [ 3. ], ans: ['Estimate the probabilities for  ⁇ UNK> ']
ERROR:root:sen: Estimate the probabilities for <UNK> from its counts just like any other regular word in the training set., ans: ['Estimate the probabilities for  ⁇ UNK> from its counts ']
ERROR:root:sen: The second alternative, in situations where we don't have a prior vocabulary in advance, is to create such a vocabulary implicitly, replacing words in the training data by <UNK> based on their frequency., ans: [' ⁇ UNK> ']
ERROR:root:sen: In either case we then proceed to train the language model as before, treating <UNK> like a regular word., ans: [' ⁇ UNK> ']
ERROR:root:sen: Laplace smoothing does not perform well enough to be used Laplace smoothing in modern n-gram models, but it usefully introduces many of the concepts that we see in other smoothing algorithms, gives a useful baseline, and is also a practical smoothing algorithm for other tasks like text classification (Chapter 4)., ans: ['Text classification ']
ERROR:root:sen: Figure 3 .6 shows the add-one smoothed counts for the bigrams in Figure [ 3.1 ] ., ans: ['Figure 3.6 ']
ERROR:root:sen: Figure 3 .7 shows the add-one smoothed probabilities for the bigrams in Figure 3 .2., ans: ['Figure 3.7 ']
ERROR:root:sen: Figure 3 .8 shows the reconstructed counts., ans: ['Figure 3.8 ']
ERROR:root:sen: .05?, ans: ['.01 ']
ERROR:root:sen: How are these λ values set?, ans: ['how are these  ⁇  values set ']
ERROR:root:sen: In addition to this explicit discount factor, we'll need a function α to distribute this probability mass to the lower order n-grams., ans: [' ⁇  ']
ERROR:root:sen: Figure 3 .9 from Church and Gale (1991) shows these counts for bigrams with c from 0 to 9., ans: ['Figure 3.9 ']
ERROR:root:sen: The first term is the discounted bigram, and the second term is the unigram with an interpolation weight λ ., ans: ['the discounted bigram ', ' the unigram with an interpolation weight  ⁇  ']
ERROR:root:sen: In other words, instead of P(w), which answers the question "How likely is w?, ans: ['P CONTINUATION ']
ERROR:root:sen: What if the horses are equally likely?, ans: ['equallength ']
ERROR:root:sen: . . , w n }., ans: ['w n  ⁇  ']
ERROR:root:sen: Extracting consumer or public sentiment is thus relevant for fields from marketing to politics., ans: ['extracting consumer or public sentiment ']
ERROR:root:sen: A part-of-speech tagger (Chapter 8) classifies each occurrence of a word in a sentence as, e.g., a noun or a verb., ans: ['chapter 8) ']
ERROR:root:sen: Formally, the task of supervised classification is to take an input x and a fixed set of output classes Y = y 1 , y 2 , ..., y M and return a predicted class y ∈ Y ., ans: ['y  ⁇  Y ']
ERROR:root:sen: The intuition of the classifier is shown in Figure 4 .1., ans: ['Figure 4.1 ']
ERROR:root:sen: Figure 4 .1 Intuition of the multinomial naive Bayes classifier applied to a movie review., ans: ['Figure 4.1 ']
ERROR:root:sen: In Eq., ans: ['Eq. 1, ']
ERROR:root:sen: Unfortunately, Eq., ans: ['Too hard ']
ERROR:root:sen: Then:, ans: ['Ndoc ']
ERROR:root:sen: Figure 4 .2 shows the final algorithm., ans: ['Figure 4.2 ']
ERROR:root:sen: Figure 4 .3 shows an example in which a set of four documents (shortened and text-normalized for this example) are remapped to binary, with the modified counts shown in the table on the right., ans: ['Figure 4.3 ']
ERROR:root:sen: Values of β > 1 favor recall, while values of β < 1 favor precision., ans: [' ⁇  > 1 ']
ERROR:root:sen: When β = 1, precision and recall are equally balanced; this is the most frequently used metric, and is called F β =1 or just F 1 :, ans: ['F  ⁇  =1 ']
ERROR:root:sen: Consider the sample confusion matrix for a hypothetical 3-way one-of email categorization decision (urgent, normal, spam) shown in Figure 4 .5., ans: ['Figure 4.5 ']
ERROR:root:sen: Figure 4 .6 shows the confusion matrix for each class separately, and shows the computation of microaveraged and macroaveraged precision., ans: ['Figure 4.6 ']
ERROR:root:sen: Now we choose one of those k folds as a test set, train our folds classifier on the remaining k − 1 folds, and then compute the error rate on the test set., ans: ['k  ⁇  1 folds ']
ERROR:root:sen: What to do?, ans: ['create a fixed training set and test set ']
ERROR:root:sen: Now we ask how likely is it, if the null hypothesis H 0 was correct, that among these test sets we would encounter the value of δ (x) that we found., ans: [' ⁇  (x) ']
ERROR:root:sen: But if δ (x) is very small, it might be less surprising to us even if H 0 were true and A is not really better than B, and so the p-value would be higher., ans: ['if  ⁇  (x) is very small ']
ERROR:root:sen: It is common to use values like .05 or .01 as the thresholds., ans: ['.05 or.01 ']
ERROR:root:sen: We say that a result (e.g., "A is better than B") is statistically significant if statistically significant the δ we saw has a probability that is below the threshold and we therefore reject this null hypothesis., ans: ['if statistically significant the  ⁇  we saw has a probability that is below the threshold ']
ERROR:root:sen: Figure 4 .8 shows a couple examples., ans: ['Figure 4.8 ']
ERROR:root:sen: Assuming H 0 (A isn't better than B), we would expect that δ (X), estimated over many test sets, would be zero; a much higher value would be surprising, since H 0 specifically assumes A isn't better than B., ans: [' ⁇  (X), estimated over many test sets, would be zero ']
ERROR:root:sen: Calculate δ (x) # how much better does algorithm A do than B on x s = 0 for i = 1 to b do for j = 1 to n do # Draw a bootstrap sample x (i) of size n Select a member of x at random and add it to, ans: ['a member of x at random and add it to the bootstrap sample x (i) of size n ']
ERROR:root:sen: The pattern [ /[b-g]/ ] specifies one of the characters b, c, d, e, f, or g. Some other examples are shown in Figure [ 2.3 ]., ans: ['Figure [ 2.3] ']
ERROR:root:sen: The square braces can also be used to specify what a single character cannot be, by use of the caret ˆ., ans: ['caret  ⁇  ']
ERROR:root:sen: If the caret ˆ is the first symbol after the open square brace [, the resulting pattern is negated., ans: ['the caret  ⁇  is the first symbol after the open square brace ']
ERROR:root:sen: baaa!, ans: ['baaaa! ']
ERROR:root:sen: baaaaa!, ans: ['baa! baaa! baaaa! ']
ERROR:root:sen: . . ., ans: ['sheep ']
ERROR:root:sen: The most common anchors are the caretˆand the dollar sign $., ans: ['caret ⁇ and the dollar sign $ ']
ERROR:root:sen: There are also two other anchors: \b matches a word boundary, and \B matches a non-boundary., ans: [' ⁇ b ']
ERROR:root:sen: The operator *?, ans: ['Kleene star ']
ERROR:root:sen: The operator +?, ans: ['Kleene plus ']
ERROR:root:sen: Suppose we wanted to do this without the use of [ /\b/ ]., ans: ['[ / ⁇ b/ ]. ']
ERROR:root:sen: The regular expression [ /{3}/ ] means "exactly 3 occurrences of the previous character or expression"., ans: ['exact 3 occurrences of the previous character or expression ']
ERROR:root:sen: So /a\., ans: ['/a ⁇  ']
ERROR:root:sen: REs for counting are summarized in Figure 2 .9., ans: ['Figure 2.9 ']
ERROR:root:sen: The most common of these are the newline character newline \n and the tab character \t., ans: ['newline character newline  ⁇ n and the tab character  ⁇ t ']
ERROR:root:sen: We’ll add a decimal point and two digits afterwards: /$[0-9]+\., ans: ['/$[0-9]+ ⁇  ']
ERROR:root:sen: optional and to make sure we’re at a word boundary: /(ˆ|\W)$[0-9]+(\., ans: ['/( ⁇ | ⁇ W)$[0-9]+( ⁇.] ']
ERROR:root:sen: One last catch!, ans: ['$199999.99 ']
ERROR:root:sen: )\b/, ans: ['GB ']
ERROR:root:sen: Here's how it looks: [ s[ /([0-9]+)/ ]<\1>/ ], ans: ['[ s[ /([0-9]+)/ ] ⁇ 1>/ ']
ERROR:root:sen: We do this by surrounding the first X with the parenthesis operator, and replacing the second X with the number operator \1, as follows: /the (., ans: [' ⁇ 1 ']
ERROR:root:sen: *)er they were, the \1er they will be/, ans: [' ⁇ 1er they will be ']
ERROR:root:sen: *)er they (., ans: ['they (. *)er they (. *) ']
ERROR:root:sen: Similarly, the third capture group is stored in \\3, the fourth is \\4, and so on., ans: [' ⁇ 4 ']
ERROR:root:sen: /(?, ans: ['/(?) ']
ERROR:root:sen: ELIZA 3 : YOUR BOYFRIEND MADE YOU COME HERE User 4 : He says I'm depressed much of the time., ans: ['DEPRESSED ']
ERROR:root:sen: Input lines are [ 2.2 ] \u2022 WORDS 11 first uppercased., ans: ['[ 2.2 ]  ⁇ u2022 WORDS 11 ']
ERROR:root:sen: The operator (?!, ans: ['pattern ']
ERROR:root:sen: What do you think they are?, ans: ['what do you think they are ']
ERROR:root:sen: Figure 2 .11 shows the rough numbers of types and tokens computed from some popular English corpora., ans: ['Figure 2.11 ']
ERROR:root:sen: [ 2.1 ], where k and β are positive constants, and 0 < β < 1., ans: ['k and  ⁇  ']
ERROR:root:sen: The value of β depends on the corpus size and the genre, but at least for the large corpora in Figure 2 .11, β ranges from .67 to .75., ans: ['.67 to.75 ']
ERROR:root:sen: How can a user of a dataset know all these details?, ans: ['how can a user of a dataset know all these details ']
ERROR:root:sen: Motivation: Why was the corpus collected, by whom, and who funded it?, ans: ['why ']
ERROR:root:sen: Situation: When and in what situation was the text written/spoken?, ans: ['when and in what situation ']
ERROR:root:sen: Normalizing word formats 3., ans: ['normalizing ']
ERROR:root:sen: The algorithm is based on series of rewrite rules run in series, as a cascade, in cascade which the output of each pass is fed as input to the next pass; here is a sampling of the rules: ATIONAL → ATE (e.g., relational → relate) ING → if stem contains vowel (e.g., motoring → motor) SSES → SS (e.g., grasses → grass), ans: ['SSES  ⁇  SS ', ' a cascade ']
ERROR:root:sen: Figure 2 .14 Representing the minimum edit distance between two strings as an alignment., ans: ['Figure 2.14 ']
ERROR:root:sen: Imagine some string (perhaps it is exention) that is in this optimal path (whatever it is)., ans: ['imagine some string ']
ERROR:root:sen: Why?, ans: ['shorter ']
ERROR:root:sen: The algorithm is summarized in Figure [ 2.17 ]; Figure [ 2.18 ] shows the results of applying the algorithm to the distance between intention and execution with the version of Levenshtein in Eq., ans: ['Figure [ 2.17] ', ' Levenshtein ']
ERROR:root:sen: Figure 2 .19 shows this path with the boldfaced cell., ans: ['Figure 2.19 ']
ERROR:root:sen: Figure 2 .19 also shows the intuition of how to compute this alignment path., ans: ['Figure 2.19 ']
ERROR:root:sen: While we worked our example with simple Levenshtein distance, the algorithm in Figure 2 .17 allows arbitrary weights on the operations., ans: ['Figure 2.17 ']
ERROR:root:sen: For the joint probability of each word in a sequence having a particular value P(X = w 1 ,Y = w 2 , Z = w 3 , ...,W = w n ) we'll use P(w 1 , w 2 , ..., w n )., ans: ['P(w 1, w 2,..., w n) ']
ERROR:root:sen: Now how can we compute probabilities of entire sequences like P(w 1 , w 2 , ..., w n )?, ans: ['P(w 1, w 2,..., w n) ']
ERROR:root:sen: We don't know any way to compute the exact probability of a word given a long sequence of preceding words, P(w n |w n−1 1 )., ans: ['P(w n |w n ⁇ 1 1 ']
ERROR:root:sen: Given the bigram assumption for the probability of an individual word, we can compute the probability of a complete word sequence by substituting Eq., ans: ['substituting Eq. [ 3.7] into Eq. [ 3.4] ']
ERROR:root:sen: We can simplify this equation, since the sum of all bigram counts that start with a given word w n−1 must be equal to the unigram count for that word w n−1 (the reader should take a moment to be convinced of this):, ans: ['the unigram count ', ' the sum of all bigram counts that start with a given word w n ⁇ 1 ']
ERROR:root:sen: We'll first need to augment each sentence with a special symbol <s> at the beginning of the sentence, to give us the bigram context of the first word., ans: [' ⁇ s> ']
ERROR:root:sen: The MLE of its probability is 400 1000000 or .0004., ans: ['400 1000000 or.0004 ']
ERROR:root:sen: Figure 3 .1 shows the bigram counts from a piece of a bigram grammar from the Berkeley Restaurant Project., ans: ['Figure 3.1 ', ' Berkeley Restaurant Project ']
ERROR:root:sen: For example, to compute trigram probabilities at the very beginning of the sentence, we use two pseudo-words for the first trigram (i.e., P(I|<s><s>)., ans: ['P(I| ⁇ s> ⁇ s> ']
ERROR:root:sen: Adding in log space is equivalent to multiplying in linear space, so we combine log probabilities by adding them., ans: ['adding log probabilities ']
ERROR:root:sen: [ 3.15 ] or Eq., ans: ['the entire sequence of words in some test set ']
ERROR:root:sen: Since this sequence will cross many sentence boundaries, we need to include the begin-and end-sentence markers <s> and </s> in the probability computation., ans: ['begin-and-end-sentence markers ']
ERROR:root:sen: By Eq., ans: ['Eq. 3.15 ']
ERROR:root:sen: Now we see the following test set: 0 0 0 0 0 3 0 0 0 0., ans: ['0 0 0 0 0 0 3 0 0 0. ']
ERROR:root:sen: Figure 3 .3 shows a visualization, using a unigram LM computed from the text of this book., ans: ['Figure 3.3 ']
ERROR:root:sen: Figure 3 .3 A visualization of the sampling distribution for sampling sentences by repeatedly sampling unigrams., ans: ['Figure 3.3 ']
ERROR:root:sen: We can use the same technique to generate bigrams by first generating a random bigram that starts with <s> (according to its bigram probability)., ans: [' ⁇ s> ']
ERROR:root:sen: To give an intuition for the increasing power of higher-order n-grams, Figure 3 .4 shows random sentences generated from unigram, bigram, trigram, and 4gram models trained on Shakespeare's works., ans: ['Figure 3.4 ']
ERROR:root:sen: Figure 3 .5 shows sentences generated by unigram, bigram, and trigram grammars trained on 40 million words from WSJ., ans: ['Figure 3.5 ']
ERROR:root:sen: Compare these examples to the pseudo-Shakespeare in Figure 3 .4., ans: ['Figure 3.4 ']
ERROR:root:sen: These zerosthings that don't ever occur in the training set but do occur in zeros the test set-are a problem for two reasons., ans: ['zeros things ']
ERROR:root:sen: is one in which we model these potential unknown words in the test set by adding a pseudo-word called <UNK>., ans: [' ⁇ UNK> ']
ERROR:root:sen: [ 1. ], ans: ['Choose a vocabulary (word list) that is fixed in advance ']
ERROR:root:sen: [ 2. ], ans: ['any word that is not in this set ']
ERROR:root:sen: Convert in the training set any word that is not in this set (any word) to the unknown word token <UNK> in a text normalization step., ans: [' ⁇ UNK> ']
ERROR:root:sen: [ 3. ], ans: ['Estimate the probabilities for  ⁇ UNK> ']
ERROR:root:sen: Estimate the probabilities for <UNK> from its counts just like any other regular word in the training set., ans: ['Estimate the probabilities for  ⁇ UNK> from its counts ']
ERROR:root:sen: The second alternative, in situations where we don't have a prior vocabulary in advance, is to create such a vocabulary implicitly, replacing words in the training data by <UNK> based on their frequency., ans: [' ⁇ UNK> ']
ERROR:root:sen: In either case we then proceed to train the language model as before, treating <UNK> like a regular word., ans: [' ⁇ UNK> ']
ERROR:root:sen: Laplace smoothing does not perform well enough to be used Laplace smoothing in modern n-gram models, but it usefully introduces many of the concepts that we see in other smoothing algorithms, gives a useful baseline, and is also a practical smoothing algorithm for other tasks like text classification (Chapter 4)., ans: ['Text classification ']
ERROR:root:sen: Figure 3 .6 shows the add-one smoothed counts for the bigrams in Figure [ 3.1 ] ., ans: ['Figure 3.6 ']
ERROR:root:sen: Figure 3 .7 shows the add-one smoothed probabilities for the bigrams in Figure 3 .2., ans: ['Figure 3.7 ']
ERROR:root:sen: Figure 3 .8 shows the reconstructed counts., ans: ['Figure 3.8 ']
ERROR:root:sen: .05?, ans: ['.01 ']
ERROR:root:sen: How are these λ values set?, ans: ['how are these  ⁇  values set ']
ERROR:root:sen: In addition to this explicit discount factor, we'll need a function α to distribute this probability mass to the lower order n-grams., ans: [' ⁇  ']
ERROR:root:sen: Figure 3 .9 from Church and Gale (1991) shows these counts for bigrams with c from 0 to 9., ans: ['Figure 3.9 ']
ERROR:root:sen: The first term is the discounted bigram, and the second term is the unigram with an interpolation weight λ ., ans: ['the discounted bigram ', ' the unigram with an interpolation weight  ⁇  ']
ERROR:root:sen: In other words, instead of P(w), which answers the question "How likely is w?, ans: ['P CONTINUATION ']
ERROR:root:sen: What if the horses are equally likely?, ans: ['equallength ']
ERROR:root:sen: . . , w n }., ans: ['w n  ⁇  ']
ERROR:root:sen: Extracting consumer or public sentiment is thus relevant for fields from marketing to politics., ans: ['extracting consumer or public sentiment ']
ERROR:root:sen: A part-of-speech tagger (Chapter 8) classifies each occurrence of a word in a sentence as, e.g., a noun or a verb., ans: ['chapter 8) ']
ERROR:root:sen: Formally, the task of supervised classification is to take an input x and a fixed set of output classes Y = y 1 , y 2 , ..., y M and return a predicted class y ∈ Y ., ans: ['y  ⁇  Y ']
ERROR:root:sen: The intuition of the classifier is shown in Figure 4 .1., ans: ['Figure 4.1 ']
ERROR:root:sen: Figure 4 .1 Intuition of the multinomial naive Bayes classifier applied to a movie review., ans: ['Figure 4.1 ']
ERROR:root:sen: In Eq., ans: ['Eq. 1, ']
ERROR:root:sen: Unfortunately, Eq., ans: ['Too hard ']
ERROR:root:sen: Then:, ans: ['Ndoc ']
ERROR:root:sen: Figure 4 .2 shows the final algorithm., ans: ['Figure 4.2 ']
ERROR:root:sen: Figure 4 .3 shows an example in which a set of four documents (shortened and text-normalized for this example) are remapped to binary, with the modified counts shown in the table on the right., ans: ['Figure 4.3 ']
ERROR:root:sen: Values of β > 1 favor recall, while values of β < 1 favor precision., ans: [' ⁇  > 1 ']
ERROR:root:sen: When β = 1, precision and recall are equally balanced; this is the most frequently used metric, and is called F β =1 or just F 1 :, ans: ['F  ⁇  =1 ']
ERROR:root:sen: Consider the sample confusion matrix for a hypothetical 3-way one-of email categorization decision (urgent, normal, spam) shown in Figure 4 .5., ans: ['Figure 4.5 ']
ERROR:root:sen: Figure 4 .6 shows the confusion matrix for each class separately, and shows the computation of microaveraged and macroaveraged precision., ans: ['Figure 4.6 ']
ERROR:root:sen: Now we choose one of those k folds as a test set, train our folds classifier on the remaining k − 1 folds, and then compute the error rate on the test set., ans: ['k  ⁇  1 folds ']
ERROR:root:sen: What to do?, ans: ['create a fixed training set and test set ']
ERROR:root:sen: Now we ask how likely is it, if the null hypothesis H 0 was correct, that among these test sets we would encounter the value of δ (x) that we found., ans: [' ⁇  (x) ']
ERROR:root:sen: But if δ (x) is very small, it might be less surprising to us even if H 0 were true and A is not really better than B, and so the p-value would be higher., ans: ['if  ⁇  (x) is very small ']
ERROR:root:sen: It is common to use values like .05 or .01 as the thresholds., ans: ['.05 or.01 ']
ERROR:root:sen: We say that a result (e.g., "A is better than B") is statistically significant if statistically significant the δ we saw has a probability that is below the threshold and we therefore reject this null hypothesis., ans: ['if statistically significant the  ⁇  we saw has a probability that is below the threshold ']
ERROR:root:sen: Figure 4 .8 shows a couple examples., ans: ['Figure 4.8 ']
ERROR:root:sen: Assuming H 0 (A isn't better than B), we would expect that δ (X), estimated over many test sets, would be zero; a much higher value would be surprising, since H 0 specifically assumes A isn't better than B., ans: [' ⁇  (X), estimated over many test sets, would be zero ']
ERROR:root:sen: Calculate δ (x) # how much better does algorithm A do than B on x s = 0 for i = 1 to b do for j = 1 to n do # Draw a bootstrap sample x (i) of size n Select a member of x at random and add it to, ans: ['a member of x at random and add it to the bootstrap sample x (i) of size n ']
ERROR:root:sen: [ 1. ], ans: ['feature representation ']
ERROR:root:sen: [ 3. ], ans: ['objective function ']
ERROR:root:sen: [ 4. ], ans: ['algorithm for optimizing the objective function ']
ERROR:root:sen: The first is a metric for how close the current label (ŷ) is to the true gold label y., ans: ['how close the current label ( ⁇ ) is to the true gold label y ']
ERROR:root:sen: We'll call this: L(ŷ, y) = How muchŷ differs from the true y, ans: ['L( ⁇, y) = How much ⁇  differs from the true y ']
ERROR:root:sen: Eq., ans: ['Eq. [ 5.10] ']
ERROR:root:sen: [ 5.10 ]., ans: ['cross-entropy loss ']
ERROR:root:sen: Let's see if this loss function does the right thing for our example from Figure 5 .2., ans: ['Figure 5.2 ']
ERROR:root:sen: In Eq., ans: ['Eq. 5.13 ']
ERROR:root:sen: Thus to find the minimum, gradient descent tells us to go in the opposite direction: moving w in a positive direction., ans: ['move w in a positive direction ']
ERROR:root:sen: The magnitude of the amount to move in gradient descent is the value of the slope d dw L( f (x; w), y) weighted by a learning rate η., ans: ['learning rate  ⁇  ']
ERROR:root:sen: [ 5.18 ] (the interested reader can see Section [ 5.8 ] for the derivation of this equation):, ans: ['Section [ 5.8] ']
ERROR:root:sen: Note in Eq., ans: ['Eq. 5.18 ']
ERROR:root:sen: Compute the loss L(ŷ (i) , y (i) ) # How far off isŷ (i) from the true output, ans: ['L( ⁇  (i) ']
ERROR:root:sen: # How should we move θ to maximize loss?, ans: ['move  ⁇  ']
ERROR:root:sen: Step 1 (computing the loss) is used to report how well we are doing on the current tuple., ans: ['Computing the loss ']
ERROR:root:sen: We'll use a simplified version of the example in Figure 5 .2 as it sees a single observation x, whose correct value is y = 1 (this is a positive review), and with only two features:, ans: ['y = 1 ', ' Figure 5.2 ']
ERROR:root:sen: In our mini example there are three parameters, so the gradient vector has 3 dimensions, for w 1 , w 2 , and b., ans: ['w 1, w 2, and b ', ' three ']
ERROR:root:sen: Now that we have a gradient, we compute the new parameter vector θ 1 by moving θ 0 in the opposite direction from the gradient:, ans: ['moving  ⁇  0 in the opposite direction ']
ERROR:root:sen: So after one step of gradient descent, the weights have shifted to be: w 1 = .15, w 2 = .1, and b = .05., ans: ['w 1 =.15 ', ' w 2 =.1, and b =.05. ']
ERROR:root:sen: So after one step of gradient descent, the weights have shifted to be: w 1 = .15, w 2 = .1, and b = .05., ans: ['w 1 =.15 ', ' w 2 =.1, and b =.05. ']
ERROR:root:sen: To avoid overfitting, a new regularization term R(θ ) is added to the objective regularization function in Eq., ans: ['R( ⁇ ) ']
ERROR:root:sen: The new regularization term R(θ ) is used to penalize large weights., ans: ['R( ⁇  ) ']
ERROR:root:sen: Thus a setting of the weights that matches the training data perfectly-but uses many weights with high values to do so-will be penalized more than a setting that matches the data a little less well, but does so using smaller weights., ans: ['sets of the weights that matches the training data perfectly ']
ERROR:root:sen: L2 regularization corresponds to assuming that weights are distributed according to a Gaussian distribution with mean µ = 0., ans: ['Gaussian distribution with mean  ⁇  = 0. ']
ERROR:root:sen: The softmax function softmax takes a vector z = [z 1 , z 2 , ..., z k ] of k arbitrary values and maps them to a probability distribution, with each value in the range (0,1), and all the values summing to 1., ans: ['z = [z 1, z 2,..., z k ] ']
ERROR:root:sen: :, ans: ['true class ']
ERROR:root:sen: Here we'll use the notation w k to mean the vector of weights from each input x i to the output node k, and the indicator function 1{}, which evaluates to 1 if the condition in the brackets is true and to 0 otherwise., ans: ['1 ⁇  ', ' w k ']
ERROR:root:sen: And now plugging in the derivative of the sigmoid, and using the chain rule one more time, we end up with Eq., ans: ['Eq. 5.44 ', ' plugging in the derivative of the sigmoid ', ' using the chain rule ']
ERROR:root:sen: How should we represent the meaning of a word?, ans: ['how should we represent the meaning of a word ']
ERROR:root:sen: For example the SimLex-999 dataset (Hill et al., 2015) gives values on a scale from 0 to 10, like the examples below, which range from near-synonyms (vanish, disappear) to pairs that scarcely seem to have anything in common (hole, agreement): Consider the meanings of the words coffee and cup., ans: ['Hole, agreement ']
ERROR:root:sen: Frames have semantic roles (like buyer, seller, goods, money), and words in a sentence can take on these roles., ans: ['buyers, seller, goods, money ']
ERROR:root:sen: Figure 6 .2 shows a small selection from a term-document matrix showing the occurrence of four words in four plays by Shakespeare., ans: ['Figure 6.2 ']
ERROR:root:sen: This is clear with the raw numbers; in the first dimension (battle) the comedies have low numbers and the others have high numbers, and we can see it visually in Figure 6 .4; we'll see very shortly how to quantify this intuition more formally., ans: ['Figure 6.4 ']
ERROR:root:sen: Figure 6 .8 shows a visualization., ans: ['Figure 6.8 ']
ERROR:root:sen: Fig ., ans: ['6.2 ']
ERROR:root:sen: 6 .9 applies tf-idf weighting to the Shakespeare term-document matrix in Figure 6 .2, using the tf equation Eq., ans: ['6.9 ']
ERROR:root:sen: Figure 6 .9 A tf-idf weighted term-document matrix for four words in four Shakespeare plays, using the counts in Figure [ 6.2 ] ., ans: ['Figure 6.9 ']
ERROR:root:sen: To distinguish whether two words whose individual probability is each 10 −6 occur together less often than chance, we would need to be certain that the probability of the two occurring together is significantly different than 10 −12 , and this kind of granularity would require an enormous corpus., ans: ['10  ⁇ 12 ']
ERROR:root:sen: We'll use Figure 6 .10, which repeats Figure 6 .6 plus all the count marginals, and let's pretend for ease of calculation that these are the only words/contexts that matter., ans: ['Figure 6.10 ']
ERROR:root:sen: events is to slightly change the computation for P(c), using a different function P α (c) that raises the probability of the context word to the power of α:, ans: ['raises the probability of the context word to the power of  ⁇  ']
ERROR:root:sen: Levy et al., ans: ['Levy and al. ']
ERROR:root:sen: 2015found that a setting of α = [ 0.75 ] improved performance of embeddings on a wide range of tasks (drawing on a similar weighting used for skipgrams described below in Eq., ans: [' ⁇  = [ 0.75 ] ']
ERROR:root:sen: This works because raising the count to α = [ 0.75 ] increases the probability assigned to rare contexts, and hence lowers their PMI (P α (c) > P(c) when c is rare)., ans: ['raises the count to  ⁇  = [ 0.75 ] increases the probability assigned to rare contexts ']
ERROR:root:sen: For example, we can find the 10 most similar words to any target word w by computing the cosines between w and each of the V − 1 other words, sorting, and looking at the top 10., ans: ['computing the cosines between w and each of the V  ⁇  1 other words ']
ERROR:root:sen: [ 1. ], ans: ['positive examples ']
ERROR:root:sen: [ 2. ], ans: ['Randomly sample other words in the lexicon to get negative samples ']
ERROR:root:sen: [ 3. ], ans: ['logistic regression ']
ERROR:root:sen: [ 4. ], ans: ['weights ']
ERROR:root:sen: How does the classifier compute the probability P?, ans: ['how does the classifier compute the probability P ']
ERROR:root:sen: In other words:, ans: ['in other words: ']
ERROR:root:sen: The dot product c [ • ] w is not a probability, it's just a number ranging from −∞ to ∞ (since the elements in word2vec embeddings can be negative, the dot product can be negative)., ans: ['a number ranging from  ⁇  to  ⁇  ']
ERROR:root:sen: To turn the dot product into a probability, we'll use the logistic or sigmoid function σ (x), the fundamental core of logistic regression:, ans: ['logistic or sigmoid function  ⁇  (x) ']
ERROR:root:sen: Figure 6 .13 The embeddings learned by the skipgram model., ans: ['Figure 6.13 ']
ERROR:root:sen: The noise words are chosen according to their weighted unigram frequency p α (w), where α is a weight., ans: ['according to their weighted unigram frequency p  ⁇  (w), where  ⁇  is a weight ']
ERROR:root:sen: Figure 6 .14 shows the intuition of one step of learning., ans: ['Figure 6.14 ']
ERROR:root:sen: Just as in logistic regression, then, the learning algorithm starts with randomly initialized W and C matrices, and then walks through the training corpus using gradient descent to move W and C so as to maximize the objective in Eq., ans: ['gradient descent ', ' random initialized W and C matrices ']
ERROR:root:sen: The embedding model thus seems to be extracting representations of relations like MALE-FEMALE, or CAPITAL-CITY-OF, or even COMPARATIVE/SUPERLATIVE, as shown in Figure 6 .16 from GloVe., ans: ['Figure 6.16 ']
ERROR:root:sen: 2002a , Nosek et al., ans: ['2002b ']
ERROR:root:sen: 2002b ., ans: ['Caliskan et al. 2017 ']
ERROR:root:sen: The semantic textual similarity task (Agirre et al., ans: ['Agirre and al. ']
ERROR:root:sen: 2012, Agirre et al., ans: ['2015 ']
ERROR:root:sen: and it is in this sense that we currently use the word., ans: ['embed ']
ERROR:root:sen: Collobert and Weston 2007 See Manning et al., ans: ['Manning and Weston ']
ERROR:root:sen: Given a set of inputs x 1 ...x n , a unit has bias term a set of corresponding weights w 1 ...w n and a bias b, so the weighted sum z can be represented as:, ans: ['x 1...x n ']
ERROR:root:sen: As defined in Eq., ans: ['Eq. 7.2 ']
ERROR:root:sen: Substituting Eq., ans: ['Substituting Eq. [ 7.2 ] ']
ERROR:root:sen: [ 7.2 ] into Eq., ans: ['Eq. 7.2 ']
ERROR:root:sen: 4) Figure 7 .2 shows a final schematic of a basic neural unit., ans: ['Figure 7.2 ']
ERROR:root:sen: The simplest activation function, and perhaps the most commonly used, is the rectified linear unit, also called the ReLU, shown in Figure 7 .3b., ans: ['the rectified linear unit ', ' Figure 7.3b ']
ERROR:root:sen: It's very easy to build a perceptron that can compute the logical AND and OR functions of its binary inputs; Figure 7 .4 shows the necessary weights., ans: ['Figure 7.4 ']
ERROR:root:sen: .), ans: ['decision boundary ']
ERROR:root:sen: Figure 7 .5 shows the possible logical inputs (00, 01, 10, and 11) and the line drawn by one possible set of parameters for an AND and an OR classifier., ans: ['Figure 7.5 ']
ERROR:root:sen: Figure 7 .6 shows a figure with the input being processed by two layers of neural units., ans: ['Figure 7.6 ']
ERROR:root:sen: In this example we just stipulated the weights in Figure 7 .6., ans: ['Figure 7.6 ']
ERROR:root:sen: In fact, the computation only has three steps: multiplying the weight matrix by the input vector x, adding the bias vector b, and applying the activation function g (such as the sigmoid, tanh, or ReLU activation function defined above)., ans: ['multipliing the weight matrix by the input vector x, adding the bias vector b, and applying the activation function g ']
ERROR:root:sen: And the weight matrix W has dimensionality W ∈ R n 1 ×n 0 , i.e., ans: ['W  ⁇  R n 1  ⁇ n 0 ']
ERROR:root:sen: [n 1 , n 0 ]., ans: ['[n 1, n 0 ]. ']
ERROR:root:sen: [ 7.8 ] will compute the value of each h j as σ, ans: [' ⁇  ']
ERROR:root:sen: Let's see how this happens., ans: ['U ']
ERROR:root:sen: Figure 7 .9 shows a visualization., ans: ['Figure 7.9 ']
ERROR:root:sen: We'll continue showing the bias as b when we go over the learning algorithm in Section [ 7.6 ], but then we'll switch to this simplified notation without explicit bias terms for the rest of the book., ans: ['Section [ 7.6] ', ' b ']
ERROR:root:sen: What's the next word?, ans: ["what's the next word "]
ERROR:root:sen: [0 0 0 0 1 0 0 ... 0 0 0 0] 1 2 3 4 5 6 7 ... ... |V| The feedforward neural language model (sketched in Figure 7 .13) has a moving window that can see N words into the past., ans: ['Figure 7.13 ']
ERROR:root:sen: The one-hot vector for 'for' (index 35) is Figure 7 .13 Forward inference in a feedforward neural language model., ans: ['Figure 7.13 ']
ERROR:root:sen: What the system produces, via Eq., ans: ['Eq. 7.13 ']
ERROR:root:sen: :, ans: ['true class ']
ERROR:root:sen: For deep networks, computing the gradients for each weight is much more complex, since we are computing the derivative with respect to weight parameters that appear all the way back in the very early layers of the network, even though the loss is computed only at the very end of the network., ans: ['computation the gradients for each weight ']
ERROR:root:sen: In the figure, we've assumed the inputs a = 3, b = 1, c = −2, and we've shown the result of the forward pass to compute the result L(3, 1, −2) = −10., ans: ['a = 3, b = 1, c =  ⁇ 2 ']
ERROR:root:sen: ., ans: ['input variables ']
ERROR:root:sen: Hyperparameters include the learning rate η, the mini-batch size, the model architecture (the number of layers, the number of hidden nodes per layer, the choice of activation functions), how to regularize, and so on., ans: ['hyperparameters ']
ERROR:root:sen: Now that we've seen how to train a generic neural net, let's talk about the architecture for training a neural language model, setting the parameters θ = E, W, U, b., ans: [' ⁇  = E, W, U, b ', ' neural language model ']
ERROR:root:sen: This is useful when the task the network is designed for (sentiment classification, or translation, or parsing) places strong constraints on what makes a good representation for words., ans: ['sentence classification, or translation, or parsing ']
ERROR:root:sen: to set all the parameters θ = E, W, U, b., ans: ['to set all the parameters  ⁇  = E, W, U, b ']
ERROR:root:sen: That's because over time, many different words will appear as w t−2 or w t−1 , and we'd like to just represent each word with one vector, whichever context position it appears in., ans: ['w t ⁇ 2 or w t ⁇ 1 ']
ERROR:root:sen: Recall that the general form for this (repeated from Eq., ans: ['Eq. 7.26 ']
ERROR:root:sen: This gradient can be computed in any standard neural network framework which will then backpropagate through θ = E, W, U, b., ans: [' ⁇  = E, W, U, b ', ' any standard neural network framework ']
ERROR:root:sen: They can preposition indicate spatial or temporal relations, whether literal (on it, before then, by the house) or metaphorical (on time, with gusto, beside herself), and relations like marking the agent in Hamlet was written by Shakespeare., ans: ['Mark the agent ']
ERROR:root:sen: . . )., ans: ['Auxiliary verbs ']
ERROR:root:sen: Figure 8 .5 shows typical generic named entity types., ans: ['Figure 8.5 ']
ERROR:root:sen: is trained to label each token in a text with tags that indicate the presence (or absence) of particular kinds of named entities., ans: ['labels that indicate the presence (or absence) of particular kinds of named entities ']
ERROR:root:sen: More formally, consider a sequence of state variables q 1 , q 2 , ..., q i ., ans: ['q 1, q 2,..., q i ']
ERROR:root:sen: ([ 8.4 ]) hot hot hot hot ([ 8.5 ]) cold hot cold hot What does the difference in these probabilities tell you about a real-world weather fact encoded in Figure 8 .8a?, ans: ['real-world weather fact encoded in Figure 8.8a ']
ERROR:root:sen: The A matrix contains the tag transition probabilities P(t i |t i−1 ) which represent the probability of a tag occurring given the previous tag., ans: ['P(t i |t i ⁇ 1 ']
ERROR:root:sen: The A transition probabilities, and B observation likelihoods of the HMM are illustrated in Figure 8 .9 for three states in an HMM part-of-speech tagger; the full tagger would have one state for each tag., ans: ['one ', ' Figure 8.9 ']
ERROR:root:sen: More formally, Figure 8 .9 An illustration of the two parts of an HMM representation: the A transition probabilities used to compute the prior probability, and the B observation likelihoods that are associated with each state, one likelihood for each possible observation word., ans: ['Figure 8.9 ']
ERROR:root:sen: Plugging the simplifying assumptions from Eq., ans: ['plugging the simplifying assumptions ']
ERROR:root:sen: [ 8.16 ] into Eq., ans: ['Eq. [ 8.16 ] ']
ERROR:root:sen: Figure 8 .11 shows an intuition of this lattice for the sentence Janet will back the bill., ans: ['Figure 8.11 ']
ERROR:root:sen: ., ans: ['Viterbi ']
ERROR:root:sen: Figure 8 .13 expresses the b i (o t ) probabilities, the observation likelihoods of words given tags., ans: ['Figure 8.13 ']
ERROR:root:sen: Figure 8 .14 The first few entries in the individual state columns for the Viterbi algorithm., ans: ['Figure 8.14 ']
ERROR:root:sen: Figure 8 .14 shows a fleshed-out version of the sketch we saw in Figure 8 .11, the Viterbi lattice for computing the best hidden state sequence for the observation sequence Janet will back the bill., ans: ['Figure 8.14 ']
ERROR:root:sen: We begin in column 1 (for the word Janet) by setting the Viterbi value in each cell to the product of the π transition probability (the start probability for that state i, which we get from the <s > entry of Figure [ 8.12 ]) , and the observation likelihood of the word Janet given the tag for that cell., ans: ['by setting the Viterbi value in each cell to the product of the  ⁇  transition probability ']
ERROR:root:sen: The reader should fill in the rest of the lattice in Figure 8 .14 and backtrace to see whether or not the Viterbi algorithm returns the gold state sequence NNP MD VB DT NN., ans: ['Figure 8.14 ']
ERROR:root:sen: 2 Again, in a linear-chain CRF, each local feature f k at position i can depend on any information from: (y i−1 , y i , X, i)., ans: ['y i ⁇ 1 ']
ERROR:root:sen: Concretely, this involves filling an N ×T array with the appropriate values, maintaining backpointers as we proceed., ans: ['N  ⁇ T array ']
ERROR:root:sen: Recall from Eq., ans: ['Eq. 8.19 ']
ERROR:root:sen: [ 1. ], ans: ['high-precision rules ']
ERROR:root:sen: [ 2. ], ans: ['substring matches ']
ERROR:root:sen: [ 3. ], ans: ['application-specific name lists ']
ERROR:root:sen: [ 4. ], ans: ['supervised sequence labeling techniques ']
ERROR:root:sen: Large vocabularies mean many unknown words, and these unknown words cause significant performance degradations in a wide variety of languages (including Czech, Slovene, Estonian, and Romanian) (Hajič, 2000) ., ans: ['large vocabularies ']
ERROR:root:sen: Highly inflectional languages also have much more information than English coded in word morphology, like case (nominative, accusative, genitive) or gender (masculine, feminine)., ans: ['highly inflectional ']
ERROR:root:sen: A wide exploration of features followed (Zhou et al., 2005) ., ans: ['Wu et al., 2005 ']
ERROR:root:sen: (2010), and ways to combine labeled and unlabeled data, for example by co-training (Clark et al., ans: ['Co-training ']
ERROR:root:sen: 2003; Søgaard 2010) ., ans: ['S ⁇ gaard ']
ERROR:root:sen: Figure 9 .1, reproduced from Chapter 7, shows a neural language model with window size 3 predicting what word follows the input for all the., ans: ['Figure 9.1 ']
ERROR:root:sen: Anything outside the context window has no impact on the decision being made., ans: ['Nothing outside the context window ']
ERROR:root:sen: For example, in Figure 9 .1 the phrase all the appears in one window in the second and third positions, and in the next window in the first and second positions, forcing the network to learn two separate patterns for what should be the same item., ans: ['Figure 9.1 ']
ERROR:root:sen: Recall from page 36 that the perplexity (PP) of a model θ on an unseen test set is the inverse probability that θ assigns to the test set, normalized by the test set length., ans: ['the inverse probability that  ⁇  assigns to the test set ']
ERROR:root:sen: Figure 9 .2 illustrates the structure of an RNN., ans: ['Figure 9.2 ']
ERROR:root:sen: To see this, consider Figure 9 .3 which clarifies the nature of the recurrence and how it factors into the computation at the hidden layer., ans: ['Figure 9.3 ']
ERROR:root:sen: Figure 9 .3 Simple recurrent neural network illustrated as a feedforward network., ans: ['Figure 9.3 ']
ERROR:root:sen: As shown in Figure 9 .3, we now have 3 sets of weights to update: W, the weights from the input layer to the hidden layer, U, the weights from the previous hidden layer to the current hidden layer, and finally V, the weights from the hidden layer to the output layer., ans: ['Figure 9.3 ']
ERROR:root:sen: Figure 9 .5 highlights two considerations that we didn't have to worry about with backpropagation in feedforward networks., ans: ['Figure 9.5 ']
ERROR:root:sen: In these cases, we can unroll the input into manageable fixed-length segments and treat each segment as a distinct training item., ans: ['separate training item ']
ERROR:root:sen: The probability of an entire sequence is just the product of the probabilities of each item in the sequence, where we'll use y i [w i ] to mean the probability of the true word w i at time step i., ans: ['the probability of the true word w i at time step i ', ' y i [w i] ']
ERROR:root:sen: Figure 9 .6 illustrates this training regimen., ans: ['Figure 9.6 ']
ERROR:root:sen: And, since the length of these embeddings corresponds to the size of the hidden layer d h , the shape of the embedding matrix E is |V | × d h ., ans: ['|V |  ⁇  d h ']
ERROR:root:sen: In sequence labeling, the network's task is to assign a label chosen from a small fixed set of labels to each element of a sequence, like the part-of-speech tagging and named entity recognition tasks from Chapter 8., ans: ['assigned from a small fixed set of labels ']
ERROR:root:sen: Figure 9 .8 illustrates this approach., ans: ['Figure 9.8 ']
ERROR:root:sen: We first randomly sample a word to begin a sequence based on its suitability as the start of a sequence., ans: ['random sample a word ']
ERROR:root:sen: [ • ] Sample a word in the output from the softmax distribution that results from using the beginning of sentence marker, <s>, as the first input., ans: [' ⁇ s> ']
ERROR:root:sen: [ • ] Continue generating until the end of sentence marker, </s>, is sampled or a fixed length limit is reached., ans: ['continue generating ']
ERROR:root:sen: Figure 9 .9 illustrates this approach., ans: ['Figure 9.9 ']
ERROR:root:sen: 15) Figure 9 .11 illustrates such a bidirectional network that concatenates the outputs of the forward and backward pass., ans: ['Figure 9.11 ']
ERROR:root:sen: ([ 9.26 ]) Figure 9 .13 illustrates the complete computation for a single LSTM unit., ans: ['Figure 9.13 ']
ERROR:root:sen: To see this, consider Figure 9 .14 which illustrates the inputs and outputs associated with each kind of unit., ans: ['Figure 9.14 ']
ERROR:root:sen: Figure 9 .15 illustrates the flow of information in a single causal, or backward looking, self-attention layer., ans: ['Figure 9.15 ']
ERROR:root:sen: Figure 9 .15 Information flow in a causal (or masked) self-attention model., ans: ['Figure 9.15 ']
ERROR:root:sen: This dot product has the right shape since both the query and the key are of dimensionality 1 × d. Let's update our previous comparison calculation to reflect this, replacing Eq., ans: ['1  ⁇  d ']
ERROR:root:sen: [ 9.32 ] with Eq., ans: ['Eq. [ 9.32 ] ']
ERROR:root:sen: We then multiply X by the key, query, and value matrices (all of dimensionality d × d) to produce matrices Q ∈ R N×d , K ∈ R N×d , and V ∈ R N×d , containing all the key, query, and value vectors:, ans: ['matrices Q  ⁇  R N ⁇ d, K  ⁇  R N ⁇ d, and V  ⁇  R N ⁇ d ']
ERROR:root:sen: Figure 9 .17 depicts the QK matrix., ans: ['Figure 9.17 ']
ERROR:root:sen: Figure 9 .18 illustrates a standard transformer block consisting of a single attention layer followed by a fully-connected feedforward layer with residual connections and layer normalizations following each., ans: ['Figure 9.18 ']
ERROR:root:sen: The first step in layer normalization is to calculate the mean, µ, and standard deviation, σ , over the elements of the vector to be normalized., ans: ['calculate the mean,  ⁇, and standard deviation,  ⁇  ']
ERROR:root:sen: Finally, in the standard implementation of layer normalization, two learnable parameters, γ and β , representing gain and offset values, are introduced., ans: [' ⁇  and  ⁇  ', ' gain and offset values ']
ERROR:root:sen: i ., ans: ['key, value, and query embeddings ']
ERROR:root:sen: Thus for each head i, we have weight layers W Q i ∈ R d×d k , W K i ∈ R d×d k , and, ans: ['W K i  ⁇  R d ⁇ d k ']
ERROR:root:sen: Figure 9 .19 illustrates this approach with 4 self-attention heads., ans: ['Figure 9.19 ']
ERROR:root:sen: How does a transformer model the position of each token in the input sequence?, ans: ['how does a transformer model the position of each token in the input sequence ']
ERROR:root:sen: This can be seen from the fact that if you scramble the order of the inputs in the attention computation in Figure 9 .16 you get exactly the same answer., ans: ['Figure 9.16 ']
ERROR:root:sen: Where do we get these positional embeddings?, ans: ['where ']
ERROR:root:sen: Figure 9 .20 shows the idea., ans: ['Figure 9.20 ']
ERROR:root:sen: Figure 9 .21 illustrates the general approach., ans: ['Figure 9.21 ']
ERROR:root:sen: Linear Layer Figure 9 .21 Training a transformer as a language model., ans: ['Linear Layer Figure 9.21 ']
ERROR:root:sen: Note the key difference between this figure and the earlier RNN-based version shown in Figure 9 .6., ans: ['Figure 9.6 ']
ERROR:root:sen: Chinese doesn't grammatically mark plurality on nouns (unlike English, which has the "-s" in "recommendations"), and so the Chinese must use the modifier 各项/various to make it clear that there is not just one recommendation., ans: [' ⁇ /various ']
ERROR:root:sen: We'll introduce the algorithm in sections Section [ 10.2 ], and in following sections give important components of the model like beam search decoding, and we'll discuss how MT is evaluated, introducing the simple chrF metric., ans: ['beam search ', ' chrF metric ', ' Section [ 10.2] ']
ERROR:root:sen: For example, Figure 10 .2 summarizes some of the complexities discussed by Hutchins and Somers (1992) in translating English leg, foot, and paw, to French., ans: ['Figure 10.2 ']
ERROR:root:sen: For example, English does not have a word that corresponds neatly to Mandarin xiào or Japanese oyakōkōo (in English one has to make do with awkward phrases like filial piety or loving child, or good son/daughter for both)., ans: ['oyak ⁇ k ⁇ o ']
ERROR:root:sen: Referentially sparse languages, like Chinese or Japanese, that require the hearer to do more inferential work to recover antecedents are also called cold languages., ans: ['Cold languages ']
ERROR:root:sen: [ 1. ], ans: ['encoder ']
ERROR:root:sen: [ 2. ], ans: ['context vector ']
ERROR:root:sen: Let's formalize and generalize this model a bit in Figure 10 .5., ans: ['Figure 10.5 ']
ERROR:root:sen: The decoder during inference uses its own estimated outputŷ t as the input for the next time step x t+1 ., ans: ['its own estimated output ⁇  t ']
ERROR:root:sen: Teacher forcing means that we force the system to use the gold target token from training as the next input x t+1 , rather than allowing it to rely on the (possibly erroneous) decoder outputŷ t . This speeds up training., ans: ['teacher forcing ']
ERROR:root:sen: The simplest such score, called dot-product attention, implements relevance as dot-product attention similarity: measuring how similar the decoder hidden state is to an encoder hidden state, by computing the dot product between them:, ans: ['dot-product attention ', ' computing the dot product between them ', ' measure how similar the decoder hidden state is to an encoder hidden state ']
ERROR:root:sen: To make use of these scores, we'll normalize them with a softmax to create a vector of weights, α i j , that tells us the proportional relevance of each encoder hidden state j to the prior hidden decoder state, h d i−1 ., ans: ['a softmax ', ' h d i ⁇ 1 ']
ERROR:root:sen: We are searching for the best action sequence, i.e., ans: ['the target string with the highest probability ']
ERROR:root:sen: Figure 10 .11 demonstrates the problem, using a made-up example., ans: ['Figure 10.11 ']
ERROR:root:sen: Figure 10 .11 A search tree for generating the target string T = t 1 ,t 2 , ... from the vocabulary V = {yes, ok, <s>}, given the source string, showing the probability of generating each token from that state., ans: ['.4*.7* ']
ERROR:root:sen: This process continues until a </s> is generated indicating that a complete candidate output has been found., ans: ['until a complete candidate output has been found ']
ERROR:root:sen: Recall from Eq., ans: ['Eq. 10.10 ']
ERROR:root:sen: Figure 10 .13 shows the scoring for the example sentence shown in Figure 10 .12, using some simple made-up probabilities., ans: ['Figure 10.13 ']
ERROR:root:sen: Figure 10 .14 gives the algorithm., ans: ['Figure 10.14 ']
ERROR:root:sen: At a high-level, the architecture, sketched in Figure 10 .15, is quite similar to what we saw for RNNs., ans: ['Figure 10.15 ']
ERROR:root:sen: [ 1. ], ans: ['Initialize the wordpiece lexicon with characters ']
ERROR:root:sen: [ 2. ], ans: ['Repeat ']
ERROR:root:sen: Figure 10 .17 gives a sample hypothetical sentence alignment., ans: ['Figure 10.17 ']
ERROR:root:sen: Using humans to evaluate is most accurate, but automatic metrics are also used for convenience., ans: ['automated metrics ', ' humans ']
ERROR:root:sen: ):, ans: ['unigram recall, bigram recall ']
ERROR:root:sen: Let's see how we computed that chrF value for HYP1 (we'll leave the computation of the chrF value for HYP2 as an exercise for the reader)., ans: ['spaces ']
ERROR:root:sen: . . , x n ) is a reference translation,x = (x 1 , ., ans: ['x = (x 1,.., x n ']
ERROR:root:sen: . . ,x m ) is a candidate machine translation, and r ∈ R is a human rating that expresses the quality ofx with respect to x., ans: ['r  ⁇  R ']
ERROR:root:sen: Machine translation raises many of the same ethical issues that we've discussed in earlier chapters., ans: ['Ethics ']
ERROR:root:sen: Figure 10 .19 shows examples from Prates et al., ans: ['Figure 10.19 ']
ERROR:root:sen: ∀ et al., ans: [' ⁇  et al. ']
ERROR:root:sen: Human evaluation is the gold standard, but automatic evaluation metrics like chrF, which measure character n-gram overlap with human translations, or more recent metrics based on embedding similarity, are also commonly used., ans: ['human evaluation ']
ERROR:root:sen: As high-quality MT proved elusive (Bar-Hillel, 1960) , there grew a consensus on the need for better evaluation and more basic research in the new fields of formal and computational linguistics., ans: ['bar-Hillel ']
ERROR:root:sen: "How much do we know at any time?, ans: ['Much more ']
ERROR:root:sen: How do children achieve this rate of vocabulary growth given their daily experiences during this period?, ans: ['how do children achieve this rate of vocabulary growth ']
ERROR:root:sen: Figure 11 .1, reproduced here from Chapter 9, illustrates the information flow in the purely left-to-right approach of Chapter 9., ans: ['Figure 11.1 ']
ERROR:root:sen: Figure 11 .1 A causal, backward looking, transformer model like Chapter 9., ans: ['Figure 11.1 ']
ERROR:root:sen: Figure 11 .2 Information flow in a bidirectional self-attention model., ans: ['Figure 11.2 ']
ERROR:root:sen: Bidirectional encoders overcome this limitation by allowing the self-attention mechanism to range over the entire input, as shown in Figure 11 .2., ans: ['Figure 11.2 ']
ERROR:root:sen: The first step is to pack the input embeddings x i into a matrix X ∈ R N×d h . That is, each row of X is the embedding of one token of the input., ans: ['X  ⁇  R N ⁇ d h ']
ERROR:root:sen: We then multiply X by the key, query, and value weight matrices (all of dimensionality d × d) to produce matrices Q ∈ R N×d , K ∈ R N×d , and V ∈ R N×d , containing all the key, query, and value vectors in a single step., ans: ['matrices Q  ⁇  R N ⁇ d, K  ⁇  R N ⁇ d, and V  ⁇  R N ⁇ d ']
ERROR:root:sen: Figure 11 .3 illustrates the result of this operation for an input with length 5., ans: ['Figure 11.3 ']
ERROR:root:sen: Finally, we can scale these scores, take the softmax, and then multiply the result by V resulting in a matrix of shape N × d where each row contains a contextualized output embedding corresponding to each token in the input., ans: ['matrix of shape N  ⁇  d ']
ERROR:root:sen: The sampled items which drive the learning process are chosen from among the set of tokenized inputs., ans: ['selected from among the set of tokenized inputs ']
ERROR:root:sen: Figure 11 .5 Masked language model training., ans: ['Figure 11.5 ']
ERROR:root:sen: Figure 11 .5 illustrates this approach with a simple example., ans: ['Figure 11.5 ']
ERROR:root:sen: With a predicted probability distribution for each masked item, we can use crossentropy to compute the loss for each masked item-the negative log probability assigned to the actual masked word, as shown in Figure 11 .5., ans: ['crossentropy ', ' Figure 11.5 ']
ERROR:root:sen: Figure 11 .6 illustrates this with one of our earlier examples., ans: ['Figure 11.6 ']
ERROR:root:sen: Figure 11 .6 Span-based language model training., ans: ['Figure 11.6 ']
ERROR:root:sen: Figure 11 .7 illustrates the overall NSP training setup., ans: ['Figure 11.7 ']
ERROR:root:sen: Figure 11 .7 An example of the NSP loss calculation., ans: ['Figure 11.7 ']
ERROR:root:sen: Figure 11 .8 illustrates this overall approach to sequence classification., ans: ['Figure 11.8 ']
ERROR:root:sen: Figure 11 .9 illustrates an example of this approach., ans: ['Figure 11.9 ']
ERROR:root:sen: ., ans: ['application-specific length limit ']
ERROR:root:sen: )., ans: ['how are these two spans related? ']
ERROR:root:sen: Figure 11 .10 A span-oriented approach to named entity classification., ans: ['Figure 11.10 ']
ERROR:root:sen: Figure 11 .10 illustrates this approach with an example., ans: ['Figure 11.10 ']
ERROR:root:sen: Extra pretraining (Gururangan et al., 2020) on non-toxic subcorpora seems to reduce a language model's tendency to generate toxic language somewhat (Gehman et al., 2020) ., ans: ['Göhman ']

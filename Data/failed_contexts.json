["The regular expression [ /[1234567890]/ ] specifies any single digit. While such classes of characters as digits or letters are important building blocks in expressions, they can get awkward (e.g., it\u2019s inconvenient to specify [ /[ABCDEFGHIJKLMNOPQRSTUVWXYZ]/ ] to mean \u201cany capital letter\u201d). In cases where there is a well-defined sequence associated with a set of characters, the brackets can be used with the dash (-) to specify any one character in a range. The pattern [ /[2-5]/ ] specifies any one of the characters 2, 3, 4, or 5. The pattern [ /[b-g]/ ] specifies one of the characters b, c, d, e, f, or g. Some other examples are shown in Figure [ 2.3 ].", "The square braces can also be used to specify what a single character cannot be, by use of the caret \u02c6. If the caret \u02c6 is the first symbol after the open square brace [, the resulting pattern is negated. For example, the pattern [ /[\u02c6a]/ ] matches any single character (including special characters) except a. This is only true when the caret is the first symbol after the open square brace. If it occurs anywhere else, it usually stands for a caret; Figure [ 2.4 ] shows some examples.", "We can think of the question mark as meaning \"zero or one instances of the previous character\". That is, it's a way of specifying how many of something that we want, something that is very important in regular expressions. For example, consider the language of certain sheep, which consists of strings that look like the following: baa! baaa! baaaa! baaaaa! . . .", "Anchors are special characters that anchor regular expressions to particular places in a string. The most common anchors are the caret\u02c6and the dollar sign $. The caret matches the start of a line. The pattern [ /\u02c6The/ ] matches the word The only at the start of a line. Thus, the caret\u02c6has three uses: to match the start of a line, to indicate a negation inside of square brackets, and just to mean a caret. (What are the contexts that allow grep or Python to know which function a given caret is supposed to have?) The dollar sign $ matches the end of a line. So the pattern $ is a useful pattern for matching a space at the end of a line, and [ /\u02c6The dog\\.$/ ] matches a line that contains only the phrase The dog. (We have to use the backslash here since we want the . to mean \"period\" and not the wildcard.)", "There are also two other anchors: \\b matches a word boundary, and \\B matches a non-boundary. Thus, [ /\\bthe\\b/ ] matches the word the but not the word other. More technically, a \"word\" for the purposes of a regular expression is defined as any sequence of digits, underscores, or letters; this is based on the definition of \"words\" in programming languages. For example, [ /\\b99\\b/ ] will match the string 99 in There are 99 bottles of beer on the wall (because 99 follows a space) but not 99 in There are 299 bottles of beer on the wall (since 99 follows a number). But it will match 99 in $99 (since 99 follows a dollar sign ($), which is not a digit, underscore, or letter).", "[ /the*/ ] matches theeeee but not thethe. Because sequences have a higher precedence than disjunction, [ /the|any/ ] matches the or any but not thany or theny. Patterns can be ambiguous in another way. Consider the expression [ /[a-z]*/ ] when matching against the text once upon a time. Since [ /[a-z]*/ ] matches zero or more letters, this expression could match nothing, or just the first letter o, on, onc, or once. In these cases regular expressions always match the largest string they can; we say that patterns are greedy, expanding to cover as much of a string as they can. There are, however, ways to enforce non-greedy matching, using another meaning of the ? qualifier. The operator *? is a Kleene star that matches as little text as possible. The operator +? is a Kleene plus that matches as little text as possible.", "Suppose we wanted to do this without the use of [ /\\b/ ]. We might want this since [ /\\b/ ] won\u2019t treat underscores and numbers as word boundaries; but we might want to find the in some context where it might also have underlines or numbers nearb (the or the25). We need to specify that we want instances in which there are no alphabetic letters on either side of the the: [ /[\u02c6a-zA-Z][tT]he[\u02c6a-zA-Z]/ ]", "Figure [ 2.8 ] shows some aliases for common ranges, which can be used mainly to save typing. Besides the Kleene * and Kleene + we can also use explicit numbers as counters, by enclosing them in curly brackets. The regular expression [ /{3}/ ] means \"exactly 3 occurrences of the previous character or expression\". So /a\\. {24}z/ will match a followed by 24 dots followed by z (but not a followed by 23 or 25 dots followed by a z).", "A range of numbers can also be specified. So [ /{n,m}/ ] specifies from n to m occurrences of the previous char or expression, and [ /{n,}/ ] means at least n occurrences of the previous expression. REs for counting are summarized in Figure 2 .9.", "Finally, certain special characters are referred to by special notation based on the backslash (\\) (see Figure 2 .10). The most common of these are the newline character newline \\n and the tab character \\t. To refer to characters that are special themselves (like ., *, [, and \\) , precede them with a backslash, (i.e., [ /\\./ ], [ /\\*/ ], [ /\\[/ ], and [ /\\\\/ ]).", "Now we just need to deal with fractions of dollars. We\u2019ll add a decimal point and two digits afterwards: /$[0-9]+\\. [0-9][0-9]/", "optional and to make sure we\u2019re at a word boundary: /(\u02c6|\\W)$[0-9]+(\\. [0-9][0-9])?\\b/", "One last catch! This pattern allows prices like $199999.99 which would be far too expensive! We need to limit the dollars: /(\u02c6|\\W)$[0-9]{0,3}(\\. [0-9][0-9])?\\b/", "How about disk space? We\u2019ll need to allow for optional fractions again ([ 5.5 ] GB); note the use of ? for making the final s optional, and the of [ / */ ] to mean \u201czero or more spaces\u201d since there might always be extra spaces lying around: /\\b[0-9]+(\\.[0-9]+)? *(GB|[Gg]igabytes? )\\b/", "It is often useful to be able to refer to a particular subpart of the string matching the first pattern. For example, suppose we wanted to put angle brackets around all integers in a text, for example, changing the 35 boxes to the <35> boxes. We'd like a way to refer to the integer we've found so that we can easily add the brackets. To do this, we put parentheses ( and ) around the first pattern and use the number operator \\1 in the second pattern to refer back. Here's how it looks: [ s[ /([0-9]+)/ ]<\\1>/ ]", "The parenthesis and number operators can also specify that a certain string or expression must occur twice in the text. For example, suppose we are looking for the pattern \"the Xer they were, the Xer they will be\", where we want to constrain the two X's to be the same string. We do this by surrounding the first X with the parenthesis operator, and replacing the second X with the number operator \\1, as follows: /the (. *)er they were, the \\1er they will be/", "This use of parentheses to store a pattern in memory is called a capture group. Every time a capture group is used (i.e., parentheses surround a pattern), the resulting match is stored in a numbered register. If you match two different sets of register parentheses, \\\\2 means whatever matched the second capture group. Thus /the (. *)er they (. *), the \\\\1er we \\\\2/ will match the faster they ran, the faster we ran but not the faster they ran, the faster we ate. Similarly, the third capture group is stored in \\\\3, the fourth is \\\\4, and so on.", "Parentheses thus have a double function in regular expressions; they are used to group terms for specifying the order in which operators should apply, and they are used to capture something in a register. Occasionally we might want to use parentheses for grouping, but don't want to capture the resulting pattern in a register. In that case we use a non-capturing group, which is specified by putting the commands non-capturing group ? : after the open paren, in the form (? : pattern). /(? :some|a few) (people|cats) like some \\\\1/ will match some cats like some cats but not some cats like some a few.", "Substitutions and capture groups are very useful in implementing simple chatbots like ELIZA (Weizenbaum, 1966). Recall that ELIZA simulates a Rogerian psychologist by carrying on conversations like the following: User 1 : Men are all alike. ELIZA 1 : IN WHAT WAY User 2 : They're always bugging us about something or other. ELIZA 2 : CAN YOU THINK OF A SPECIFIC EXAMPLE User 3 : Well, my boyfriend made me come here. ELIZA 3 : YOUR BOYFRIEND MADE YOU COME HERE User 4 : He says I'm depressed much of the time. ELIZA 4 : I AM SORRY TO HEAR YOU ARE DEPRESSED", "ELIZA works by having a series or cascade of regular expression substitutions each of which matches and changes some part of the input lines. Input lines are [ 2.2 ] \\u2022 WORDS 11 first uppercased. The first substitutions then change all instances of MY to YOUR, and I'M to YOU ARE, and so on. The next set of substitutions matches and replaces other patterns in the input. Here are some examples:", "These lookahead assertions make use of the (? syntax that we saw in the previous section for non-capture groups. The operator (?= pattern) is true if pattern occurs, but is zero-width, i.e. the match pointer doesn\u2019t advance. The operator (?! pattern) only returns true if a pattern does not match, but again is zero-width and doesn\u2019t advance the cursor. Negative lookahead is commonly used when we are parsing some complex pattern but want to rule out a special case. For example suppose we want to match, at the beginning of a line, any single word that doesn\u2019t start with \u201cVolcano\u201d. We can use negative lookahead to do this: /\u02c6(? !Volcano)[A-Za-z]+/", "But we also sometimes keep disfluencies around. Disfluencies like uh or um are actually helpful in speech recognition in predicting the upcoming word, because they may signal that the speaker is restarting the clause or idea, and so for speech recognition they are treated as regular words. Because people use different disfluencies they can also be a cue to speaker identification. In fact Clark and Fox Tree (2002) showed that uh and um have different meanings. What do you think they are?", "Figure 2 .11 shows the rough numbers of types and tokens computed from some popular English corpora. The larger the corpora we look at, the more word types we find, and in fact this relationship between the number of types |V | and number of tokens N is called Herdan's Law (Herdan, 1960) or Heaps' Law (Heaps, 1978) Herdan's Law Heaps' Law after its discoverers (in linguistics and information retrieval respectively). It is shown in Eq. [ 2.1 ], where k and \u03b2 are positive constants, and 0 < \u03b2 < 1.", "The value of \u03b2 depends on the corpus size and the genre, but at least for the large corpora in Figure 2 .11, \u03b2 ranges from .67 to .75. Roughly then we can say that the vocabulary size for a text goes up significantly faster than the square root of its length in words.", "Because language is so situated, when developing computational models for language processing from a corpus, it's important to consider who produced the language, in what context, for what purpose. How can a user of a dataset know all these details? The best way is for the corpus creator to build a datasheet (Gebru et al., 2020) or data statement (Bender and Friedman, 2018) for each corpus. A datasheet specifies properties of a dataset like:", "Motivation: Why was the corpus collected, by whom, and who funded it? Situation: When and in what situation was the text written/spoken? For example, was there a task? Was the language originally spoken conversation, edited text, social media communication, monologue vs. dialogue? Language variety: What language (including dialect/region) was the corpus in? Speaker demographics: What was, e.g., age or gender of the authors of the text? Collection process: How big is the data? If it is a subsample how was it sampled?", "Before almost any natural language processing of a text, the text has to be normalized. At least three tasks are commonly applied as part of any normalization process: 1. Tokenizing (segmenting) words 2. Normalizing word formats 3. Segmenting sentences In the next sections we walk through each of these tasks.", "The algorithm is based on series of rewrite rules run in series, as a cascade, in cascade which the output of each pass is fed as input to the next pass; here is a sampling of the rules: ATIONAL \u2192 ATE (e.g., relational \u2192 relate) ING \u2192 if stem contains vowel (e.g., motoring \u2192 motor) SSES \u2192 SS (e.g., grasses \u2192 grass)", "The gap between intention and execution, for example, is 5 (delete an i, substitute e for n, substitute x for t, insert c, substitute u for n). It's much easier to see this by looking at the most important visualization for string distances, an alignment between the two strings, shown in Figure 2 .14. Given two sequences, an alignment is a correspondence between substrings of the two sequences. Thus, we say I aligns with the empty string, N with E, and so on. Beneath the aligned strings is another representation; a series of symbols expressing an operation list for converting the top string into the bottom string: d for deletion, s for substitution, i for insertion. Figure 2 .14 Representing the minimum edit distance between two strings as an alignment.", "Imagine some string (perhaps it is exention) that is in this optimal path (whatever it is). The intuition of dynamic programming is that if exention is in the optimal operation list, then the optimal sequence must also include the optimal path from intention to exention. Why? If there were a shorter path from intention to exention, then we could use it instead, resulting in a shorter overall path, and the optimal sequence wouldn't be optimal, thus leading to a contradiction.", "The algorithm is summarized in Figure [ 2.17 ]; Figure [ 2.18 ] shows the results of applying the algorithm to the distance between intention and execution with the version of Levenshtein in Eq. [ 2.8 ].", "To extend the edit distance algorithm to produce an alignment, we can start by visualizing an alignment as a path through the edit distance matrix. Figure 2 .19 shows this path with the boldfaced cell. Each boldfaced cell represents an alignment of a pair of letters in the two strings. If two boldfaced cells occur in the same row, there will be an insertion in going from the source to the target; two boldfaced cells in the same column indicate a deletion.", "Figure 2 .19 also shows the intuition of how to compute this alignment path. The computation proceeds in two steps. In the first step, we augment the minimum edit distance algorithm to store backpointers in each cell. The backpointer from a cell points to the previous cell (or cells) that we came from in entering the current cell. We've shown a schematic of these backpointers in Figure [ 2.19 ] . Some cells have multiple backpointers because the minimum extension could have come from multiple previous cells. In the second step, we perform a backtrace. In a backtrace, we start from the last cell (at the final row and column), and follow the pointers back through the dynamic programming matrix. Each complete path between the final cell and the initial cell is a minimum distance alignment. Exercise [ 2.7 ] asks you to modify the minimum edit distance algorithm to store the pointers and compute the backtrace to output an alignment.", "While we worked our example with simple Levenshtein distance, the algorithm in Figure 2 .17 allows arbitrary weights on the operations. For spelling correction, for example, substitutions are more likely to happen between letters that are next to each other on the keyboard. The Viterbi algorithm is a probabilistic extension of minimum edit distance. Instead of computing the \"minimum edit distance\" between two strings, Viterbi computes the \"maximum probability alignment\" of one string with another. We'll discuss this more in Chapter 8.", "Similarly, if we wanted to know the joint probability of an entire sequence of words like its water is so transparent, we could do it by asking \"out of all possible sequences of five words, how many of them are its water is so transparent?\" We would have to get the count of its water is so transparent and divide by the sum of the counts of all possible five word sequences. That seems rather a lot to estimate! For this reason, we'll need to introduce more clever ways of estimating the probability of a word w given a history h, or the probability of an entire word sequence W . Let's start with a little formalizing of notation. To represent the probability of a particular random variable X i taking on the value \"the\", or P(X i = \"the\"), we will use the simplification P(the). We'll represent a sequence of N words either as w 1 . . . w n or w 1:n (so the expression w 1:n\u22121 means the string w 1 , w 2 , ..., w n\u22121 ). For the joint probability of each word in a sequence having a particular value P(X = w 1 ,Y = w 2 , Z = w 3 , ...,W = w n ) we'll use P(w 1 , w 2 , ..., w n ).", "Now how can we compute probabilities of entire sequences like P(w 1 , w 2 , ..., w n )? One thing we can do is decompose this probability using the chain rule of probability:", "The chain rule shows the link between computing the joint probability of a sequence and computing the conditional probability of a word given previous words. Equation [ 3.4 ] suggests that we could estimate the joint probability of an entire sequence of words by multiplying together a number of conditional probabilities. But using the chain rule doesn't really seem to help us! We don't know any way to compute the exact probability of a word given a long sequence of preceding words, P(w n |w n\u22121 1 ). As we said above, we can't just estimate by counting the number of times every word occurs following every long string, because language is creative and any particular context might have never occurred before!", "Given the bigram assumption for the probability of an individual word, we can compute the probability of a complete word sequence by substituting Eq. [ 3.7 ] into Eq. [ 3.4 ]:", "We can simplify this equation, since the sum of all bigram counts that start with a given word w n\u22121 must be equal to the unigram count for that word w n\u22121 (the reader should take a moment to be convinced of this):", "Let's work through an example using a mini-corpus of three sentences. We'll first need to augment each sentence with a special symbol <s> at the beginning of the sentence, to give us the bigram context of the first word. We'll also need a special end-symbol. <[ /s> 2 <s> I am Sam </ ]s> <s> Sam I am <[ /s> <s> I do not like green eggs and ham </ ]s> Here are the calculations for some of the bigram probabilities from this corpus P(I|<s>) = 2 3 = .67 P(Sam|<s>) = 1 3 = .33 P(am|I) = 2 3 = .67 P(</s>|Sam) = 1 2 = [ 0.5 ] P(Sam|am) = 1 2 = .5 P(do|I) = 1 3 = .33 For the general case of MLE n-gram parameter estimation:", "Equation [ 3.12 ] (like Eq. [ 3.11 ]) estimates the n-gram probability by dividing the observed frequency of a particular sequence by the observed frequency of a prefix. This ratio is called a relative frequency. We said above that this use of relative frequencies as a way to estimate probabilities is an example of maximum likelihood estimation or MLE. In MLE, the resulting parameter set maximizes the likelihood of the training set T given the model M (i.e., P(T |M)). For example, suppose the word Chinese occurs 400 times in a corpus of a million words like the Brown corpus. What is the probability that a random word selected from some other text of, say, a million words will be the word Chinese? The MLE of its probability is 400 1000000 or .0004. Now .0004 is not the best possible estimate of the probability of Chinese occurring in all situations; it might turn out that in some other corpus or context Chinese is a very unlikely word. But it is the probability that makes it most likely that Chinese will occur 400 times in a million-word corpus. We present ways to modify the MLE estimates slightly to get better probability estimates in Section [ 3.5 ].", "Figure 3 .1 shows the bigram counts from a piece of a bigram grammar from the Berkeley Restaurant Project. Note that the majority of the values are zero. In fact, we have chosen the sample words to cohere with each other; a matrix selected from a random set of seven words would be even more sparse .", "Some practical issues: Although for pedagogical purposes we have only described bigram models, in practice it's more common to use trigram models, which condition on the previous two words rather than the previous word, or 4-gram or even 5-gram models, when there is sufficient training data. Note that for these larger n-grams, we'll need to assume extra contexts to the left and right of the sentence end. For example, to compute trigram probabilities at the very beginning of the sentence, we use two pseudo-words for the first trigram (i.e., P(I|<s><s>).", "We always represent and compute language model probabilities in log format as log probabilities. Since probabilities are (by definition) less than or equal to 1, the more probabilities we multiply together, the smaller the product becomes. Multiplying enough n-grams together would result in numerical underflow. By using log probabilities instead of raw probabilities, we get numbers that are not as small. Adding in log space is equivalent to multiplying in linear space, so we combine log probabilities by adding them. The result of doing all computation and storage in log space is that we only need to convert back into probabilities if we need to report them at the end; then we can just take the exp of the logprob:", "In practice we don't use raw probability as our metric for evaluating language models, but a variant called perplexity. The perplexity (sometimes called PP for short) perplexity of a language model on a test set is the inverse probability of the test set, normalized by the number of words. For a test set W = w 1 w 2 . . . w N ,:", "Note that because of the inverse in Eq. [ 3.15 ], the higher the conditional probability of the word sequence, the lower the perplexity. Thus, minimizing perplexity is equivalent to maximizing the test set probability according to the language model. What we generally use for word sequence in Eq. [ 3.15 ] or Eq. [ 3.16 ] is the entire sequence of words in some test set. Since this sequence will cross many sentence boundaries, we need to include the begin-and end-sentence markers <s> and </s> in the probability computation. We also need to include the end-of-sentence marker </s> (but not the beginning-of-sentence marker <s>) in the total count of word tokens N.", "N, and assume that in the training set all the digits occurred with equal probability. By Eq. [ 3.15 ], the perplexity will be", "But suppose that the number zero is really frequent and occurs far more often than other numbers. Let's say that 0 occur 91 times in the training set, and each of the other digits occurred 1 time each. Now we see the following test set: 0 0 0 0 0 3 0 0 0 0. We should expect the perplexity of this test set to be lower since most of the time the next number will be zero, which is very predictable, i.e. has a high probability. Thus, although the branching factor is still 10, the perplexity or weighted branching factor is smaller. We leave this exact calculation as exercise 12.", "This technique of visualizing a language model by sampling was first suggested very early on by Shannon (1951) and Miller and Selfridge (1950) It's simplest to visualize how this works for the unigram case. Imagine all the words of the English language covering the probability space between 0 and 1, each word covering an interval proportional to its frequency. Figure 3 .3 shows a visualization, using a unigram LM computed from the text of this book. We choose a random value between 0 and 1, find that point on the probability line, and print the word whose interval includes this chosen value. We continue choosing random numbers and generating words until we randomly generate the sentence-final token </s>.", "Figure 3 .3 A visualization of the sampling distribution for sampling sentences by repeatedly sampling unigrams. The blue bar represents the frequency of each word. The number line shows the cumulative probabilities. If we choose a random number between 0 and 1, it will fall in an interval corresponding to some word. The expectation for the random number to fall in the larger intervals of one of the frequent words (the, of, a) is much higher than in the smaller interval of one of the rare words (polyphonic).", "We can use the same technique to generate bigrams by first generating a random bigram that starts with <s> (according to its bigram probability). Let's say the second word of that bigram is w. We next choose a random bigram starting with w (again, drawn according to its bigram probability), and so on.", "We can use the sampling method from the prior section to visualize both of these facts! To give an intuition for the increasing power of higher-order n-grams, Figure 3 .4 shows random sentences generated from unigram, bigram, trigram, and 4gram models trained on Shakespeare's works.", "To get an idea of the dependence of a grammar on its training set, let's look at an n-gram grammar trained on a completely different corpus: the Wall Street Journal (WSJ) newspaper. Shakespeare and the Wall Street Journal are both English, so we might expect some overlap between our n-grams for the two genres. Figure 3 .5 shows sentences generated by unigram, bigram, and trigram grammars trained on 40 million words from WSJ. .5 Three sentences randomly generated from three n-gram models computed from 40 million words of the Wall Street Journal, lower-casing all characters and treating punctuation as words. Output was then hand-corrected for capitalization to improve readability.", "Compare these examples to the pseudo-Shakespeare in Figure 3 .4. While they both model \"English-like sentences\", there is clearly no overlap in generated sentences, and little overlap even in small phrases. Statistical models are likely to be pretty useless as predictors if the training sets and the test sets are as different as Shakespeare and WSJ.", "But suppose our test set has phrases like: denied the offer denied the loan Our model will incorrectly estimate that the P(offer|denied the) is 0! These zerosthings that don't ever occur in the training set but do occur in zeros the test set-are a problem for two reasons. First, their presence means we are underestimating the probability of all sorts of words that might occur, which will hurt the performance of any application we want to run on this data. Second, if the probability of any word in the test set is 0, the entire probability of the test set is 0. By definition, perplexity is based on the inverse probability of the test set. Thus if some words have zero probability, we can't compute perplexity at all, since we can't divide by 0!", "In other cases we have to deal with words we haven't seen before, which we'll call unknown words, or out of vocabulary (OOV) words. is one in which we model these potential unknown words in the test set by adding a pseudo-word called <UNK>.", "[ 1. ] Choose a vocabulary (word list) that is fixed in advance. [ 2. ] Convert in the training set any word that is not in this set (any word) to the unknown word token <UNK> in a text normalization step. [ 3. ] Estimate the probabilities for <UNK> from its counts just like any other regular word in the training set.", "The second alternative, in situations where we don't have a prior vocabulary in advance, is to create such a vocabulary implicitly, replacing words in the training data by <UNK> based on their frequency. For example we can replace by <UNK> all words that occur fewer than n times in the training set, where n is some small number, or equivalently select a vocabulary size V in advance (say 50,000) and choose the top V words by frequency and replace the rest by UNK. In either case we then proceed to train the language model as before, treating <UNK> like a regular word. The exact choice of <UNK> model does have an effect on metrics like perplexity. A language model can achieve low perplexity by choosing a small vocabulary and assigning the unknown word a high probability. For this reason, perplexities should only be compared across language models with the same vocabularies (Buck et al., 2014).", "The simplest way to do smoothing is to add one to all the n-gram counts, before we normalize them into probabilities. All the counts that used to be zero will now have a count of 1, the counts of 1 will be 2, and so on. This algorithm is called Laplace smoothing. Laplace smoothing does not perform well enough to be used Laplace smoothing in modern n-gram models, but it usefully introduces many of the concepts that we see in other smoothing algorithms, gives a useful baseline, and is also a practical smoothing algorithm for other tasks like text classification (Chapter 4).", "Now that we have the intuition for the unigram case, let's smooth our Berkeley Restaurant Project bigrams. Figure 3 .6 shows the add-one smoothed counts for the bigrams in Figure [ 3.1 ] . Figure 3 .7 shows the add-one smoothed probabilities for the bigrams in Figure 3 .2. Recall that normal bigram probabilities are computed by normalizing each row of counts by the unigram count:", "It is often convenient to reconstruct the count matrix so we can see how much a smoothing algorithm has changed the original counts. These adjusted counts can be computed by Eq. [ 3.24 ]. Figure 3 .8 shows the reconstructed counts.", "One alternative to add-one smoothing is to move a bit less of the probability mass from the seen to the unseen events. Instead of adding 1 to each count, we add a fractional count k (.5? .05? .01?). This algorithm is therefore called add-k smoothing.", "How are these \u03bb values set? Both the simple interpolation and conditional interpolation \u03bb s are learned from a held-out corpus. A held-out corpus is an additional held-out training corpus that we use to set hyperparameters like these \u03bb values, by choosing the \u03bb values that maximize the likelihood of the held-out corpus. That is, we fix the n-gram probabilities and then search for the \u03bb values that-when plugged into Eq. [ 3.26 ]-give us the highest probability of the held-out set. There are various ways to find this optimal set of \u03bb s. One way is to use the EM algorithm, an iterative learning algorithm that converges on locally optimal \u03bb s (Jelinek and Mercer, 1980) .", "In order for a backoff model to give a correct probability distribution, we have to discount the higher-order n-grams to save some probability mass for the lower discount order n-grams. Just as with add-one smoothing, if the higher-order n-grams aren't discounted and we just used the undiscounted MLE probability, then as soon as we replaced an n-gram which has zero probability with a lower-order n-gram, we would be adding probability mass, and the total probability assigned to all possible strings by the language model would be greater than 1! In addition to this explicit discount factor, we'll need a function \u03b1 to distribute this probability mass to the lower order n-grams.", "To see this, we can use a clever idea from Church and Gale (1991) . Consider an n-gram that has count 4. We need to discount this count by some amount. But how much should we discount it? Church and Gale's clever idea was to look at a held-out corpus and just see what the count is for all those bigrams that had count 4 in the training set. They computed a bigram grammar from 22 million words of AP newswire and then checked the counts of each of these bigrams in another 22 million words. On average, a bigram that occurred 4 times in the first 22 million words occurred [ 3.23 ] times in the next 22 million words. Figure 3 .9 from Church and Gale (1991) shows these counts for bigrams with c from 0 to 9. .9 For all bigrams in 22 million words of AP newswire of count 0, 1, 2,...,9, the counts of these bigrams in a held-out corpus also of 22 million words.", "The first term is the discounted bigram, and the second term is the unigram with an interpolation weight \u03bb . We could just set all the d values to .75, or we could keep a separate discount value of [ 0.5 ] for the bigrams with counts of 1.", "In other words, instead of P(w), which answers the question \"How likely is w? \", we'd like to create a unigram model that we might call P CONTINUATION , which answers the question \"How likely is w to appear as a novel continuation?\". How can we estimate this probability of seeing the word w as a novel continuation, in a new unseen context? The Kneser-Ney intuition is to base our estimate of P CONTINUATION on the number of different contexts word w has appeared in, that is, the number of bigram types it completes. Every bigram type was a novel continuation the first time it was seen. We hypothesize that words that have appeared in more contexts in the past are more likely to appear in some new context as well. The number of times a word w appears as a novel continuation can be expressed as:", "What if the horses are equally likely? We saw above that if we used an equallength binary code for the horse numbers, each horse took 3 bits to code, so the average was 3. Is the entropy the same? In this case each horse would have a probability of 1 8 . The entropy of the choice of horses is then", "Until now we have been computing the entropy of a single variable. But most of what we will use entropy for involves sequences. For a grammar, for example, we will be computing the entropy of some sequence of words W = {w 1 , w 2 , . . . , w n }. One way to do this is to have a variable that ranges over sequences of words. For example we can compute the entropy of a random variable that ranges over all finite sequences of words of length n in some language L as follows:", "But to measure the true entropy of a language, we need to consider sequences of infinite length. If we think of a language as a stochastic process L that produces a sequence of words, and allow W to represent the sequence of words w 1 , . . . , w n , then L's entropy rate H(L) is defined as", "We focus on one common text categorization task, sentiment analysis, the extraction of sentiment, the positive or negative orientation that a writer expresses toward some object. A review of a movie, book, or product on the web expresses the author's sentiment toward the product, while an editorial or political text expresses sentiment toward a candidate or political action. Extracting consumer or public sentiment is thus relevant for fields from marketing to politics. The simplest version of sentiment analysis is a binary classification task, and the words of the review provide excellent cues. Consider, for example, the following phrases extracted from positive and negative reviews of movies and restaurants. Words like great, richly, awesome, and pathetic, and awful and ridiculously are very informative cues: + ...zany characters and richly applied satire, and some great plot twists \u2212 It was pathetic. The worst part about it was the boxing scenes... + ...awesome caramel sauce and sweet toasty almonds. I love this place! \u2212 ...awful pizza and ridiculously overpriced... Spam detection is another important commercial application, the binary classpam detection sification task of assigning an email to one of the two classes spam or not-spam. Many lexical and other features can be used to perform this classification. For example you might quite reasonably be suspicious of an email containing phrases like \"online pharmaceutical\" or \"WITHOUT ANY COST\" or \"Dear Winner\".", "Classification is essential for tasks below the level of the document as well. We've already seen period disambiguation (deciding if a period is the end of a sentence or part of a word), and word tokenization (deciding if a character should be a word boundary). Even language modeling can be viewed as classification: each word can be thought of as a class, and so predicting the next word is classifying the context-so-far into a class for each next word. A part-of-speech tagger (Chapter 8) classifies each occurrence of a word in a sentence as, e.g., a noun or a verb.", "Rules can be fragile, however, as situations or data change over time, and for some tasks humans aren't necessarily good at coming up with the rules. Most cases of classification in language processing are instead done via supervised machine learning, and this will be the subject of the remainder of this chapter. In supervised supervised machine learning learning, we have a data set of input observations, each associated with some correct output (a 'supervision signal'). The goal of the algorithm is to learn how to map from a new observation to a correct output. Formally, the task of supervised classification is to take an input x and a fixed set of output classes Y = y 1 , y 2 , ..., y M and return a predicted class y \u2208 Y . For text classification, we'll sometimes talk about c (for \"class\") instead of y as our output variable, and d (for \"document\") instead of x as our input variable. In the supervised situation we have a training set of N documents that have each been hand-labeled with a class:", "In this section we introduce the multinomial naive Bayes classifier, so called because it is a Bayesian classifier that makes a simplifying (naive) assumption about how the features interact. The intuition of the classifier is shown in Figure 4 .1. We represent a text document as if it were a bag-of-words, that is, an unordered set of words with their position bag-of-words ignored, keeping only their frequency in the document. In the example in the figure, instead of representing the word order in all the phrases like \"I love this movie\" and \"I would recommend it\", we simply note that the word I occurred 5 times in the entire excerpt, the word it 6 times, the words love, recommend, and movie once, and so on.", "Figure 4 .1 Intuition of the multinomial naive Bayes classifier applied to a movie review. The position of the words is ignored (the bag of words assumption) and we make use of the frequency of each word.", "Naive Bayes is a probabilistic classifier, meaning that for a document d, out of all classes c \u2208 C the classifier returns the class\u0109 which has the maximum posterior probability given the document. In Eq. [ 4.1 ] we use the hat notation\u02c6to mean \"our estimate of the correct class\".", "Unfortunately, Eq. [ 4.6 ] is still too hard to compute directly: without some simplifying assumptions, estimating the probability of every possible combination of features (for example, every possible set of words and positions) would require huge numbers of parameters and impossibly large training sets. Naive Bayes classifiers therefore make two simplifying assumptions.", "How can we learn the probabilities P(c) and P( f i |c)? Let's first consider the maximum likelihood estimate. We'll simply use the frequencies in the data. For the class prior P(c) we ask what percentage of the documents in our training set are in each class c. Let N c be the number of documents in our training data with class c and N doc be the total number of documents. Then:", "Finally, some systems choose to completely ignore another class of words: stop words, very frequent words like the and a. This can be done by sorting the vocabustop words lary by frequency in the training set, and defining the top 10-100 vocabulary entries as stop words, or alternatively by using one of the many predefined stop word lists available online. Then each instance of these stop words is simply removed from both training and test documents as if it had never occurred. In most text classification applications, however, using a stop word list doesn't improve performance, and so it is more common to make use of the entire vocabulary and not use a stop word list. Figure 4 .2 shows the final algorithm.", "While standard naive Bayes text classification can work well for sentiment analysis, some small changes are generally employed that improve performance. First, for sentiment classification and a number of other text classification tasks, whether a word occurs or not seems to matter more than its frequency. Thus it often improves performance to clip the word counts in each document at 1 (see the end of the chapter for pointers to these results). This variant is called binary multinomial naive Bayes or binary NB. The variant uses the same Eq. [ 4.10 ] except binary NB that for each document we remove all duplicate words before concatenating them into the single big document. Figure 4 .3 shows an example in which a set of four documents (shortened and text-normalized for this example) are remapped to binary, with the modified counts shown in the table on the right. The example is worked without add-1 smoothing to make the differences clearer. Note that the results counts need not be 1; the word great has a count of 2 even for Binary NB, because it appears in multiple documents.", "\u03b2 2 P + R The \u03b2 parameter differentially weights the importance of recall and precision, based perhaps on the needs of an application. Values of \u03b2 > 1 favor recall, while values of \u03b2 < 1 favor precision. When \u03b2 = 1, precision and recall are equally balanced; this is the most frequently used metric, and is called F \u03b2 =1 or just F 1 :", "Up to now we have been describing text classification tasks with only two classes. But lots of classification tasks in language processing have more than two classes. For sentiment analysis we generally have 3 classes (positive, negative, neutral) and even more classes are common for tasks like part-of-speech tagging, word sense disambiguation, semantic role labeling, emotion detection, and so on. Luckily the naive Bayes algorithm is already a multi-class classification algorithm. But we'll need to slightly modify our definitions of precision and recall. Consider the sample confusion matrix for a hypothetical 3-way one-of email categorization decision (urgent, normal, spam) shown in Figure 4 .5. The matrix shows, for example, that the system mistakenly labeled one spam document as urgent, and we have shown how to compute a distinct precision and recall value for each class. In order to derive a single metric that tells us how well the system is doing, we can combine these values in two ways. In macroaveraging, we compute the performance macroaveraging for each class, and then average over classes. In microaveraging, we collect the demicroaveraging cisions for all classes into a single confusion matrix, and then compute precision and recall from that table. Figure 4 .6 shows the confusion matrix for each class separately, and shows the computation of microaveraged and macroaveraged precision.", "In cross-validation, we choose a number k, and partition our data into k disjoint subsets called folds. Now we choose one of those k folds as a test set, train our folds classifier on the remaining k \u2212 1 folds, and then compute the error rate on the test set. Then we repeat with another fold as the test set, again training on the other k \u2212 1 folds. We do this sampling process k times and average the test set error rate from these k runs to get an average error rate. If we choose k = 10, we would train 10 different models (each on 90% of our data), test the model 10 times, and average these 10 values. This is called 10-fold cross-validation.", "The only problem with cross-validation is that because all the data is used for testing, we need the whole corpus to be blind; we can't examine any of the data to suggest possible features and in general see what's going on, because we'd be peeking at the test set, and such cheating would cause us to overestimate the performance of our system. However, looking at the corpus to understand what's going on is important in designing NLP systems! What to do? For this reason, it is common to create a fixed training set and test set, then do 10-fold cross-validation inside the training set, but compute error rate the normal way in the test set, as shown in Figure 4 .7.", "We do this by creating a random variable X ranging over all test sets. Now we ask how likely is it, if the null hypothesis H 0 was correct, that among these test sets we would encounter the value of \u03b4 (x) that we found. We formalize this likelihood as the p-value: the probability, assuming the null hypothesis H 0 is true, of seeing p-value the \u03b4 (x) that we saw or one even greater", "So in our example, this p-value is the probability that we would see \u03b4 (x) assuming A is not better than B. If \u03b4 (x) is huge (let's say A has a very respectable F 1 of .9 and B has a terrible F 1 of only .2 on x), we might be surprised, since that would be extremely unlikely to occur if H 0 were in fact true, and so the p-value would be low (unlikely to have such a large \u03b4 if A is in fact not better than B). But if \u03b4 (x) is very small, it might be less surprising to us even if H 0 were true and A is not really better than B, and so the p-value would be higher.", "A very small p-value means that the difference we observed is very unlikely under the null hypothesis, and we can reject the null hypothesis. What counts as very small? It is common to use values like .05 or .01 as the thresholds. A value of .01 means that if the p-value (the probability of observing the \u03b4 we saw assuming H 0 is true) is less than .01, we reject the null hypothesis and assume that A is indeed better than B. We say that a result (e.g., \"A is better than B\") is statistically significant if statistically significant the \u03b4 we saw has a probability that is below the threshold and we therefore reject this null hypothesis. How do we compute this probability we need for the p-value? In NLP we generally don't use simple parametric tests like t-tests or ANOVAs that you might be familiar with. Parametric tests make assumptions about the distributions of the test statistic (such as normality) that don't generally hold in our cases. So in NLP we usually use non-parametric tests based on sampling: we artificially create many versions of the experimental setup. For example, if we had lots of different test sets x we could just measure all the \u03b4 (x ) for all the x . That gives us a distribution. Now we set a threshold (like .01) and if we see in this distribution that 99% or more of those deltas are smaller than the delta we observed, i.e. that p-value(x)-the probability of seeing a \u03b4 (x) as big as the one we saw, is less than .01, then we can reject the null hypothesis and agree that \u03b4 (x) was a sufficiently surprising difference and A is really a better algorithm than B.", "Consider a tiny text classification example with a test set x of 10 documents. The first row of Figure 4 .8 shows the results of two classifiers (A and B) on this test set, with each document labeled by one of the four possibilities: (A and B both right, both wrong, A right and B wrong, A wrong and B right); a slash through a letter ( B) means that that classifier got the answer wrong. On the first document both A and B get the correct class (AB), while on the second document A got it right but B got it wrong (A B). If we assume for simplicity that our metric is accuracy, A has an accuracy of .70 and B of .50, so \u03b4 (x) is .20. Now we create a large number b (perhaps 10 5 ) of virtual test sets x (i) , each of size n = 10. Figure 4 .8 shows a couple examples. To create each virtual test set x (i) , we repeatedly (n = 10 times) select a cell from row x with replacement. For example, to create the first cell of the first virtual test set x (1) , if we happened to randomly select the second cell of the x row; we would copy the value A B into our new cell, and move on to create the second cell of x (1) , each time sampling (randomly choosing) from the original x with replacement. 1 2 3 4 5 6 7 8 9 10 A% B% \u03b4 () Now that we have the b test sets, providing a sampling distribution, we can do statistics on how often A has an accidental advantage. There are various ways to compute this advantage; here we follow the version laid out in Berg-Kirkpatrick et al. (2012) . Assuming H 0 (A isn't better than B), we would expect that \u03b4 (X), estimated over many test sets, would be zero; a much higher value would be surprising, since H 0 specifically assumes A isn't better than B. To measure exactly how surprising is our observed \u03b4 (x) we would in other circumstances compute the p-value by counting over many test sets how often \u03b4 (x (i) ) exceeds the expected zero value by \u03b4 (x) or more:", "Calculate \u03b4 (x) # how much better does algorithm A do than B on x s = 0 for i = 1 to b do for j = 1 to n do # Draw a bootstrap sample x (i) of size n Select a member of x at random and add it to", "[ 1. ] A feature representation of the input. For each input observation x (i) , this will be a vector of features [x 1 , x 2 , ..., x n ]. We will generally refer to feature i for input x ( j) as x ( j)i , sometimes simplified as x i , but we will also see the notation f i , f i (x), or, for multiclass classification, f i (c, x).", "[ 3. ] An objective function for learning, usually involving minimizing error on training examples. We will introduce the cross-entropy loss function.", "[ 4. ] An algorithm for optimizing the objective function. We introduce the stochastic gradient descent algorithm.", "Let's have an example. Suppose we are doing binary sentiment classification on movie review text, and we would like to know whether to assign the sentiment class + or \u2212 to a review document doc. We'll represent each input observation by the 6 features x 1 . . . x 6 of the input shown in the following x 3 1 if \"no\" \u2208 doc 0 otherwise 1", "This requires two components that we foreshadowed in the introduction to the chapter. The first is a metric for how close the current label (\u0177) is to the true gold label y. Rather than measure similarity, we usually talk about the opposite of this: the distance between the system output and the gold output, and we call this distance the loss function or the cost function. In the next section we'll introduce the loss loss function that is commonly used for logistic regression and also for neural networks,", "We need a loss function that expresses, for an observation x, how close the classifier output (\u0177 = \u03c3 (w [ \u2022 ] x + b)) is to the correct output (y, which is 0 or 1). We'll call this: L(\u0177, y) = How much\u0177 differs from the true y", "Eq. [ 5.10 ] describes a log likelihood that should be maximized. In order to turn this into loss function (something that we need to minimize), we'll just flip the sign on Eq. [ 5.10 ]. The result is the cross-entropy loss L CE :", "Let's see if this loss function does the right thing for our example from Figure 5 .2. We want the loss to be smaller if the model's estimate is close to correct, and bigger if the model is confused. So first let's suppose the correct gold label for the sentiment example in Figure 5 .2 is positive, i.e., y = 1. In this case our model is doing well, since from Eq. [ 5.7 ] it indeed gave the example a higher probability of being positive (.70) than negative (.30). If we plug \u03c3 (w [ \u2022 ] x + b) = .70 and y = 1 into Eq. [ 5.12 ], the right side of the equation drops out, leading to the following loss (we'll use log to mean natural log when the base is not specified):", "Our goal with gradient descent is to find the optimal weights: minimize the loss function we've defined for the model. In Eq. [ 5.13 ] below, we'll explicitly represent the fact that the loss function L is parameterized by the weights, which we'll refer to in machine learning in general as \u03b8 (in the case of logistic regression \u03b8 = w, b). So the goal is to find the set of weights which minimizes the loss function, averaged over all examples:\u03b8", "The gradient descent algorithm answers this question by finding the gradient gradient of the loss function at the current point and moving in the opposite direction. The gradient of a function of many variables is a vector pointing in the direction of the greatest increase in a function. The gradient is a multi-variable generalization of the slope, so for a function of one variable like the one in Figure 5 .3, we can informally think of the gradient as the slope. The dotted line in Figure 5 .3 shows the slope of this hypothetical loss function at point w = w 1 . You can see that the slope of this dotted line is negative. Thus to find the minimum, gradient descent tells us to go in the opposite direction: moving w in a positive direction. The magnitude of the amount to move in gradient descent is the value of the slope d dw L( f (x; w), y) weighted by a learning rate \u03b7. A higher (faster) learning learning rate rate means that we should move w more on each step. The change we make in our parameter is the learning rate times the gradient (or the slope, in our single-variable example):", "It turns out that the derivative of this function for one observation vector x is Eq. [ 5.18 ] (the interested reader can see Section [ 5.8 ] for the derivation of this equation):", "Note in Eq. [ 5.18 ] that the gradient with respect to a single weight w j represents a very intuitive value: the difference between the true y and our estimated\u0177 = \u03c3 (w [ \u2022 ] x + b) for that observation, multiplied by the corresponding input value x j .", "y is the set of training outputs (labels) y (1) , y (2) , ..., y (m) \u03b8 \u2190 0 repeat til done # see caption For each training tuple (x (i) , y (i) ) (in random order) 1. Optional (for reporting): # How are we doing on this tuple? Compute\u0177 (i) = f (x (i) ; \u03b8 ) # What is our estimated output\u0177? Compute the loss L(\u0177 (i) , y (i) ) # How far off is\u0177 (i) from the true output", "# How should we move \u03b8 to maximize loss? 3. \u03b8 \u2190 \u03b8 \u2212 \u03b7 g # Go the other way instead return \u03b8 Figure 5 .5 The stochastic gradient descent algorithm.", "Step 1 (computing the loss) is used to report how well we are doing on the current tuple. The algorithm can terminate when it converges (or when the gradient norm < ), or when progress halts (for example when the loss starts going up on a held-out set).", "Let's walk though a single step of the gradient descent algorithm. We'll use a simplified version of the example in Figure 5 .2 as it sees a single observation x, whose correct value is y = 1 (this is a positive review), and with only two features:", "In our mini example there are three parameters, so the gradient vector has 3 dimensions, for w 1 , w 2 , and b. We can compute the first gradient as follows:", "Now that we have a gradient, we compute the new parameter vector \u03b8 1 by moving \u03b8 0 in the opposite direction from the gradient:", "So after one step of gradient descent, the weights have shifted to be: w 1 = .15, w 2 = .1, and b = .05.", "To avoid overfitting, a new regularization term R(\u03b8 ) is added to the objective regularization function in Eq. [ 5.13 ], resulting in the following objective for a batch of m examples (slightly rewritten from Eq. [ 5.13 ] to be maximizing log probability rather than minimizing loss, and removing the 1 m term which doesn't affect the argmax):", "The new regularization term R(\u03b8 ) is used to penalize large weights. Thus a setting of the weights that matches the training data perfectly-but uses many weights with high values to do so-will be penalized more than a setting that matches the data a little less well, but does so using smaller weights. There are two common ways to compute this regularization term R(\u03b8 ). L2 regularization is a quadratic function of", "Both L1 and L2 regularization have Bayesian interpretations as constraints on the prior of how weights should look. L1 regularization can be viewed as a Laplace prior on the weights. L2 regularization corresponds to assuming that weights are distributed according to a Gaussian distribution with mean \u00b5 = 0. In a Gaussian or normal distribution, the further away a value is from the mean, the lower its probability (scaled by the variance \u03c3 ). By using a Gaussian prior on the weights, we are saying that weights prefer to have the value 0. A Gaussian for a weight \u03b8 j is 1", "The multinomial logistic classifier uses a generalization of the sigmoid, called the softmax function, to compute the probability p(y = c|x). The softmax function softmax takes a vector z = [z 1 , z 2 , ..., z k ] of k arbitrary values and maps them to a probability distribution, with each value in the range (0,1), and all the values summing to 1. Like the sigmoid, it is an exponential function.", "Because only one class (let's call it i) is the correct one, the vector y takes the value 1 only for this value of k, i.e., has y i = 1 and y j = 0 \u2200 j = i. A vector like this, with one value=1 and the rest 0, is called a one-hot vector. The terms in the sum in Eq. [ 5.34 ] will thus be 0 except for the term corresponding to the true class, i.e. :", "Here we'll use the notation w k to mean the vector of weights from each input x i to the output node k, and the indicator function 1{}, which evaluates to 1 if the condition in the brackets is true and to 0 otherwise. Hence the cross-entropy loss is simply the log of the output probability corresponding to the correct class, and we therefore also call this the negative log likelihood loss:", "And now plugging in the derivative of the sigmoid, and using the chain rule one more time, we end up with Eq. [ 5.44 ]:", "Let's begin by introducing some basic principles of word meaning. How should we represent the meaning of a word? In the n-gram models of Chapter 3, and in classical NLP applications, our only representation of a word is as a string of letters, or an index in a vocabulary list. This representation is not that different from a tradition in philosophy, perhaps you've seen it in introductory logic classes, in which the meaning of words is represented by just spelling the word with small capital letters; representing the meaning of \"dog\" as DOG, and \"cat\" as CAT.", "Word Similarity While words don't have many synonyms, most words do have lots of similar words. Cat is not a synonym of dog, but cats and dogs are certainly similar words. In moving from synonymy to similarity, it will be useful to shift from talking about relations between word senses (like synonymy) to relations between words (like similarity). Dealing with words avoids having to commit to a particular representation of word senses, which will turn out to simplify our task. The notion of word similarity is very useful in larger semantic tasks. Knowing similarity how similar two words are can help in computing how similar the meaning of two phrases or sentences are, a very important component of tasks like question answering, paraphrasing, and summarization. One way of getting values for word similarity is to ask humans to judge how similar one word is to another. A number of datasets have resulted from such experiments. For example the SimLex-999 dataset (Hill et al., 2015) gives values on a scale from 0 to 10, like the examples below, which range from near-synonyms (vanish, disappear) to pairs that scarcely seem to have anything in common (hole, agreement): Consider the meanings of the words coffee and cup. Coffee is not similar to cup; they share practically no features (coffee is a plant or a beverage, while a cup is a manufactured object with a particular shape). But coffee and cup are clearly related; they are associated by co-participating in an everyday event (the event of drinking coffee out of a cup). Similarly scalpel and surgeon are not similar but are related eventively (a surgeon tends to make use of a scalpel).", "Semantic Frames and Roles Closely related to semantic fields is the idea of a semantic frame. A semantic frame is a set of words that denote perspectives or semantic frame participants in a particular type of event. A commercial transaction, for example, is a kind of event in which one entity trades money to another entity in return for some good or service, after which the good changes hands or perhaps the service is performed. This event can be encoded lexically by using verbs like buy (the event from the perspective of the buyer), sell (from the perspective of the seller), pay (focusing on the monetary aspect), or nouns like buyer. Frames have semantic roles (like buyer, seller, goods, money), and words in a sentence can take on these roles.", "In a term-document matrix, each row represents a word in the vocabulary and each term-document matrix column represents a document from some collection of documents. Figure 6 .2 shows a small selection from a term-document matrix showing the occurrence of four words in four plays by Shakespeare. Each cell in this matrix represents the number of times a particular word (defined by the row) occurs in a particular document (defined by the column). Thus fool appeared 58 times in Twelfth Night.", "We can think of the vector for a document as a point in |V |-dimensional space; thus the documents in Figure 6 .3 are points in 4-dimensional space. Since 4-dimensional spaces are hard to visualize, Figure 6 .4 shows a visualization in two dimensions; we've arbitrarily chosen the dimensions corresponding to the words battle and fool. Term-document matrices were originally defined as a means of finding similar documents for the task of document information retrieval. Two documents that are similar will tend to have similar words, and if two documents have similar words their column vectors will tend to be similar. The vectors for the comedies As You Like It [1, 114, 36, 20] and Twelfth Night [0,80,58,15] look a lot more like each other (more fools and wit than battles) than they look like Julius Caesar [7, 62, 1, 2] or Henry V [13, 89, 4, 3] . This is clear with the raw numbers; in the first dimension (battle) the comedies have low numbers and the others have high numbers, and we can see it visually in Figure 6 .4; we'll see very shortly how to quantify this intuition more formally.", "Let's see how the cosine computes which of the words cherry or digital is closer in meaning to information, just using raw counts from the following shortened The model decides that information is way closer to digital than it is to cherry, a result that seems sensible. Figure 6 .8 shows a visualization.", "Fig . 6 .9 applies tf-idf weighting to the Shakespeare term-document matrix in Figure 6 .2, using the tf equation Eq. [ 6.12 ]. Note that the tf-idf values for the dimension corresponding to the word good have now all become 0; since this word appears in every document, the tf-idf weighting leads it to be ignored. Similarly, the word fool, which appears in 36 out of the 37 plays, has a much lower weight. Figure 6 .9 A tf-idf weighted term-document matrix for four words in four Shakespeare plays, using the counts in Figure [ 6.2 ] . For example the [ 0.049 ] value for wit in As You Like It is the product of tf = log 10 (20 + 1) = [ 1.322 ] and idf = .037. Note that the idf weighting has eliminated the importance of the ubiquitous word good and vastly reduced the impact of the almost-ubiquitous word fool.", "PMI values range from negative to positive infinity. But negative PMI values (which imply things are co-occurring less often than we would expect by chance) tend to be unreliable unless our corpora are enormous. To distinguish whether two words whose individual probability is each 10 \u22126 occur together less often than chance, we would need to be certain that the probability of the two occurring together is significantly different than 10 \u221212 , and this kind of granularity would require an enormous corpus. Furthermore it's not clear whether it's even possible to evaluate such scores of 'unrelatedness' with human judgments. For this reason it is more common to use Positive PMI (called PPMI) which replaces all negative PPMI PMI values with zero (Church and Hanks 1989 , Dagan et al. 1993 , Niwa and Nitta 1994 5 :", "Let's see some PPMI calculations. We'll use Figure 6 .10, which repeats Figure 6 .6 plus all the count marginals, and let's pretend for ease of calculation that these are the only words/contexts that matter. Figure 6 .10 Co-occurrence counts for four words in 5 contexts in the Wikipedia corpus, together with the marginals, pretending for the purpose of this calculation that no other words/contexts matter.", "PMI has the problem of being biased toward infrequent events; very rare words tend to have very high PMI values. One way to reduce this bias toward low frequency Figure 6 .12 The PPMI matrix showing the association between words and context words, computed from the counts in Figure 6 .11. Note that most of the 0 PPMI values are ones that had a negative PMI; for example PMI(cherry,computer) = -[ 6.7 ], meaning that cherry and computer co-occur on Wikipedia less often than we would expect by chance, and with PPMI we replace negative values by zero. events is to slightly change the computation for P(c), using a different function P \u03b1 (c) that raises the probability of the context word to the power of \u03b1:", "Levy et al. 2015found that a setting of \u03b1 = [ 0.75 ] improved performance of embeddings on a wide range of tasks (drawing on a similar weighting used for skipgrams described below in Eq. [ 6.32 ]). This works because raising the count to \u03b1 = [ 0.75 ] increases the probability assigned to rare contexts, and hence lowers their PMI (P \u03b1 (c) > P(c) when c is rare).", "Given two documents, we can then compute their document vectors d 1 and d 2 , and estimate the similarity between the two documents by cos (d 1 , d 2 ) . Document similarity is also useful for all sorts of applications; information retrieval, plagiarism detection, news recommender systems, and even for digital humanities tasks like comparing different versions of a text to see which are similar to each other. Either the PPMI model or the tf-idf model can be used to compute word similarity, for tasks like finding word paraphrases, tracking changes in word meaning, or automatically discovering meanings of words in different corpora. For example, we can find the 10 most similar words to any target word w by computing the cosines between w and each of the V \u2212 1 other words, sorting, and looking at the top 10.", "[ 1. ] Treat the target word and a neighboring context word as positive examples. [ 2. ] Randomly sample other words in the lexicon to get negative samples. [ 3. ] Use logistic regression to train a classifier to distinguish those two cases. [ 4. ] Use the learned weights as the embeddings.", "How does the classifier compute the probability P? The intuition of the skipgram model is to base this probability on embedding similarity: a word is likely to occur near the target if its embedding vector is similar to the target embedding. To compute similarity between these dense embeddings, we rely on the intuition that two vectors are similar if they have a high dot product (after all, cosine is just a normalized dot product). In other words:", "The dot product c [ \u2022 ] w is not a probability, it's just a number ranging from \u2212\u221e to \u221e (since the elements in word2vec embeddings can be negative, the dot product can be negative). To turn the dot product into a probability, we'll use the logistic or sigmoid function \u03c3 (x), the fundamental core of logistic regression:", "In summary, skip-gram trains a probabilistic classifier that, given a test target word w and its context window of L words c 1:L , assigns a probability based on how similar this context window is to the target word. The probability is based on applying the logistic (sigmoid) function to the dot product of the embeddings of the target word with each context word. To compute this probability, we just need embeddings for each target word and context word in the vocabulary. Figure 6 .13 The embeddings learned by the skipgram model. The algorithm stores two embeddings for each word, the target embedding (sometimes called the input embedding) and the context embedding (sometimes called the output embedding). The parameter \u03b8 that the algorithm learns is thus a matrix of 2|V | vectors, each of dimension d, formed by concatenating two matrices, the target embeddings W and the context+noise embeddings C. Figure 6 .13 shows the intuition of the parameters we'll need. Skip-gram actually stores two embeddings for each word, one for the word as a target, and one for the word considered as context. Thus the parameters we need to learn are two matrices W and C, each containing an embedding for every one of the |V | words in the vocabulary V . 6 Let's now turn to learning these embeddings (which is the real goal of training this classifier in the first place).", "The noise words are chosen according to their weighted unigram frequency p \u03b1 (w), where \u03b1 is a weight. If we were sampling according to unweighted frequency p(w), it would mean that with unigram probability p(\"the\") we would choose the word the as a noise word, with unigram probability p(\"aardvark\") we would choose aardvark, and so on. But in practice it is common to set \u03b1 = .75, i.e. use the weighting p 3 4 (w):", "We minimize this loss function using stochastic gradient descent. Figure 6 .14 shows the intuition of one step of learning.", "Just as in logistic regression, then, the learning algorithm starts with randomly initialized W and C matrices, and then walks through the training corpus using gradient descent to move W and C so as to maximize the objective in Eq. [ 6.34 ] by making the updates in (Eq. [ 6.39 ])-(Eq. [ 6.40 ]).", "In early work with sparse embeddings, scholars showed that sparse vector mod- Rome. The embedding model thus seems to be extracting representations of relations like MALE-FEMALE, or CAPITAL-CITY-OF, or even COMPARATIVE/SUPERLATIVE, as shown in Figure 6 .16 from GloVe. For a a : b :: a * : b * problem, meaning the algorithm is given vectors a, b, and a * and must find b * , the parallelogram method is thus:", "in the United States have been shown to associate African-American names with unpleasant words (more than European-American names), male names more with mathematics and female names with the arts, and old people's names with unpleasant words (Greenwald et al. 1998 , Nosek et al. 2002a , Nosek et al. 2002b . Caliskan et al. 2017replicated all these findings of implicit associations using GloVe vectors and cosine similarity instead of human latencies. For example African-American names like 'Leroy' and 'Shaniqua' had a higher GloVe cosine with unpleasant words while European-American names ('Brad', 'Greg', 'Courtney') had a higher cosine with pleasant words. These problems with embeddings are an example of a representational harm (Crawford 2017, Blodgett et al. 2020), which is a harm caused by representational harm a system demeaning or even ignoring some social groups. Any embedding-aware algorithm that made use of word sentiment could thus exacerbate bias against African Americans. Recent research focuses on ways to try to remove these kinds of biases, for example by developing a transformation of the embedding space that removes gender stereotypes but preserves definitional gender (Bolukbasi et al. 2016 , Zhao et al. 2017 or changing the training procedure (Zhao et al., 2018b). However, although these sorts of debiasing may reduce bias in embeddings, they do not eliminate it Historical embeddings are also being used to measure biases in the past. Garg et al. 2018used embeddings from historical texts to measure the association between embeddings for occupations and embeddings for names of various ethnicities or genders (for example the relative cosine similarity of women's names versus men's to occupation words like 'librarian' or 'carpenter') across the 20th century. They found that the cosines correlate with the empirical historical percentages of women or ethnic groups in those occupations. Historical embeddings also replicated old surveys of ethnic stereotypes; the tendency of experimental participants in 1933 to associate adjectives like 'industrious' or 'superstitious' with, e.g., Chinese ethnicity, correlates with the cosine between Chinese last names and those adjectives using embeddings trained on 1930s text. They also were able to document historical gender biases, such as the fact that embeddings for adjectives related to competence ('smart', 'wise', 'thoughtful', 'resourceful') had a higher cosine with male than female words, and showed that this bias has been slowly decreasing since 1960. We return in later chapters to this question about the role of bias in natural language processing.", "Slightly more realistic are intrinsic similarity tasks that include context. The Stanford Contextual Word Similarity (SCWS) dataset (Huang et al., 2012) and the Word-in-Context (WiC) dataset (Pilehvar and Camacho-Collados, 2019) offer richer evaluation scenarios. SCWS gives human judgments on 2,003 pairs of words in their sentential context, while WiC gives target words in two sentential contexts that are either in the same or different senses; see Section [ 18.5 ].3. The semantic textual similarity task (Agirre et al. 2012, Agirre et al. 2015) evaluates the performance of sentence-level similarity algorithms, consisting of a set of pairs of sentences, each pair with human-labeled similarity scores.", "The idea of vector semantics arose out of research in the 1950s in three distinct fields: linguistics, psychology, and computer science, each of which contributed a fundamental aspect of the model. The idea that meaning is related to the distribution of words in context was widespread in linguistic theory of the 1950s, among distributionalists like Zellig Harris, Martin Joos, and J. R. Firth, and semioticians like Thomas Sebeok. As Joos (1950) put it, the linguist's \"meaning\" of a morpheme. . . is by definition the set of conditional probabilities of its occurrence in context with all other morphemes.", "The LSA community seems to have first used the word \"embedding\" in Landauer et al. (1997) , in a variant of its mathematical meaning as a mapping from one space or mathematical structure to another. In LSA, the word embedding seems to have described the mapping from the space of sparse count vectors to the latent space of SVD dense vectors. Although the word thus originally meant the mapping from one space to another, it has metonymically shifted to mean the resulting dense vector in the latent space. and it is in this sense that we currently use the word.", "By the next decade, Bengio et al. (2003) and Bengio et al. (2006) showed that neural language models could also be used to develop embeddings as part of the task of word prediction. Collobert and Weston 2007 See Manning et al. (2008) for a deeper understanding of the role of vectors in information retrieval, including how to compare queries with documents, more details on tf-idf, and issues of scaling to very large datasets. See Kim (2019) for a clear and comprehensive tutorial on word2vec. Cruse 2004 is a useful introductory linguistic text of lexical semantics.", "At its heart, a neural unit is taking a weighted sum of its inputs, with one additional term in the sum called a bias term. Given a set of inputs x 1 ...x n , a unit has bias term a set of corresponding weights w 1 ...w n and a bias b, so the weighted sum z can be represented as:", "As defined in Eq. [ 7.2 ], z is just a real valued number. Finally, instead of using z, a linear function of x, as the output, neural units apply a non-linear function f to z. We will refer to the output of this function as the activation value for the unit, a. Since we are just modeling a single unit, the activation activation for the node is in fact the final output of the network, which we'll generally call y. So the value y is defined as:", "The sigmoid (shown in Figure 7 .1) has a number of advantages; it maps the output into the range [0, 1], which is useful in squashing outliers toward 0 or 1. And it's differentiable, which as we saw in Section [ 5.8 ] will be handy for learning. Substituting Eq. [ 7.2 ] into Eq. [ 7.3 ] gives us the output of a neural unit:", "4) Figure 7 .2 shows a final schematic of a basic neural unit. In this example the unit takes 3 input values x 1 , x 2 , and x 3 , and computes a weighted sum, multiplying each value by a weight (w 1 , w 2 , and w 3 , respectively), adds them to a bias term b, and then passes the resulting sum through a sigmoid function to result in a number between 0 and 1.", "The simplest activation function, and perhaps the most commonly used, is the rectified linear unit, also called the ReLU, shown in Figure 7 .3b. It's just the same as z ReLU when z is positive, and 0 otherwise:", "It's very easy to build a perceptron that can compute the logical AND and OR functions of its binary inputs; Figure 7 .4 shows the necessary weights. It turns out, however, that it's not possible to build a perceptron to compute logical XOR! (It's worth spending a moment to give it a try!)", ".) This line acts as a decision boundary in two-dimensional space in which the output 0 is assigned to all decision boundary inputs lying on one side of the line, and the output 1 to all input points lying on the other side of the line. If we had more than 2 inputs, the decision boundary becomes a hyperplane instead of a line, but the idea is the same, separating the space into two categories. Figure 7 .5 shows the possible logical inputs (00, 01, 10, and 11) and the line drawn by one possible set of parameters for an AND and an OR classifier. Notice that there is simply no way to draw a line that separates the positive cases of XOR (01 and 10) from the negative cases (00 and 11). We say that XOR is not a linearly separable linearly separable function. Of course we could draw a boundary with a curve, or some other function, but not a single line.", "While the XOR function cannot be calculated by a single perceptron, it can be calculated by a layered network of units. Let's see an example of how to do this from Goodfellow et al. (2016) that computes XOR using two layers of ReLU-based units. Figure 7 .6 shows a figure with the input being processed by two layers of neural units. The middle layer (called h) has two units, and the output layer (called y) has one unit. A set of weights and biases are shown for each ReLU that correctly computes the XOR function.", "In this example we just stipulated the weights in Figure 7 .6. But for real examples the weights for neural networks are learned automatically using the error backpropagation algorithm to be introduced in Section [ 7.6 ]. That means the hidden layers will learn to form useful representations. This intuition, that neural networks can automatically learn useful representations of the input, is one of their key advantages, and one that we will return to again and again in later chapters.", "The advantage of using a single matrix W for the weights of the entire layer is that now the hidden layer computation for a feedforward network can be done very efficiently with simple matrix operations. In fact, the computation only has three steps: multiplying the weight matrix by the input vector x, adding the bias vector b, and applying the activation function g (such as the sigmoid, tanh, or ReLU activation function defined above).", "Let's introduce some constants to represent the dimensionalities of these vectors and matrices. We'll refer to the input layer as layer 0 of the network, and have n 0 represent the number of inputs, so x is a vector of real numbers of dimension n 0 , or more formally x \u2208 R n 0 , a column vector of dimensionality [n 0 , 1]. Let's call the hidden layer layer 1 and the output layer layer 2. The hidden layer has dimensionality n 1 , so h \u2208 R n 1 and also b \u2208 R n 1 (since each hidden unit can take a different bias value). And the weight matrix W has dimensionality W \u2208 R n 1 \u00d7n 0 , i.e. [n 1 , n 0 ].", "Take a moment to convince yourself that the matrix multiplication in Eq. [ 7.8 ] will compute the value of each h j as \u03c3", "Let's see how this happens. Like the hidden layer, the output layer has a weight matrix (let's call it U), but some models don't include a bias vector b in the output layer, so we'll simplify by eliminating the bias vector in this example. The weight matrix is multiplied by its input vector (h) to produce the intermediate output z. z = Uh There are n 2 output nodes, so z \u2208 R n 2 , weight matrix U has dimensionality U \u2208 R n 2 \u00d7n 1 , and element U i j is the weight from unit j in the hidden layer to unit i in the output layer.", "But now instead of our vector x having n values: x = x 1 , . . . , x n , it will have n + 1 values, with a new 0th dummy value x 0 = 1: x = x 0 , . . . , x n 0 . And instead of computing each h j as follows:", "where the value W j0 replaces what had been b j . Figure 7 .9 shows a visualization. Figure 7 .9 Replacing the bias node (shown in a) with x 0 (b).", "We'll continue showing the bias as b when we go over the learning algorithm in Section [ 7.6 ], but then we'll switch to this simplified notation without explicit bias terms for the rest of the book.", "I have to make sure that the cat gets fed. but have never seen the words \"gets fed\" after the word \"dog\". Our test set has the prefix \"I forgot to make sure that the dog gets\". What's the next word? An n-gram language model will predict \"fed\" after \"that the cat gets\", but not after \"that the dog gets\". But a neural LM, knowing that \"cat\" and \"dog\" have similar embeddings, will be able to generalize from the \"cat\" context to assign a high enough probability to \"fed\" even after seeing \"dog\".", "[0 0 0 0 1 0 0 ... 0 0 0 0] 1 2 3 4 5 6 7 ... ... |V| The feedforward neural language model (sketched in Figure 7 .13) has a moving window that can see N words into the past. We'll let N-3, so the 3 words w t\u22121 , w t\u22122 , and w t\u22123 are each represented as a one-hot vector. We then multiply these one-hot vectors by the embedding matrix E. The embedding weight matrix E has a column for each word, each a column vector of d dimensions, and hence has dimensionality d \u00d7 |V |. Multiplying by a one-hot vector that has only one non-zero element x i = 1 simply selects out the relevant column vector for word i, resulting in the embedding for word i, as shown in Figure 7 .12.", "Here's the algorithm in detail for our mini example: 1. Select three embeddings from E: Given the three previous words, we look up their indices, create 3 one-hot vectors, and then multiply each by the embedding matrix E. Consider w t\u22123 . The one-hot vector for 'for' (index 35) is Figure 7 .13 Forward inference in a feedforward neural language model. At each timestep t the network computes a d-dimensional embedding for each context word (by multiplying a one-hot vector by the embedding matrix E), and concatenates the 3 resulting embeddings to get the embedding layer e. The embedding vector e is multiplied by a weight matrix W and then an activation function is applied element-wise to produce the hidden layer h, which is then multiplied by another weight matrix U. Finally, a softmax output layer predicts at each node i the probability that the next word w t will be vocabulary word V i .", "A feedforward neural net is an instance of supervised machine learning in which we know the correct output y for each observation x. What the system produces, via Eq. [ 7.13 ], is\u0177, the system's estimate of the true y. The goal of the training procedure is to learn parameters W [i] and b [i] for each layer i that make\u0177 for each training observation as close as possible to the true y.", "We can simplify this equation further. Assume this is a hard classification task, meaning that only one class is the correct one, and that there is one output unit in y for each class. If the true class is i, then y is a vector where y i = 1 and y j = 0 \u2200 j = i. A vector like this, with one value=1 and the rest 0, is called a one-hot vector. The terms in the sum in Eq. [ 7.24 ] will be 0 except for the term corresponding to the true class, i.e. :", "But these derivatives only give correct updates for one weight layer: the last one! For deep networks, computing the gradients for each weight is much more complex, since we are computing the derivative with respect to weight parameters that appear all the way back in the very early layers of the network, even though the loss is computed only at the very end of the network. The solution to computing this gradient is an algorithm called error backpropagation or backprop (Rumelhart et al., 1986) . While backprop was invented spe-error backpropagation cially for neural networks, it turns out to be the same as a more general procedure called backward differentiation, which depends on the notion of computation graphs. Let's see how that works in the next subsection.", "We can now represent this as a graph, with nodes for each operation, and directed edges showing the outputs from each operation as the inputs to the next, as in Figure 7 .14. The simplest use of computation graphs is to compute the value of the function with some given inputs. In the figure, we've assumed the inputs a = 3, b = 1, c = \u22122, and we've shown the result of the forward pass to compute the result L(3, 1, \u22122) = \u221210. In the forward pass of a computation graph, we apply each operation left to right, passing the outputs of each computation as the input to the next node.", ". And so on, until we have annotated the graph all the way to all the input variables. The forward pass conveniently already will have computed the values of the forward intermediate variables we need (like d and e) to compute these derivatives. Figure [ 7.16 ] shows the backward pass.", "Various forms of regularization are used to prevent overfitting. One of the most important is dropout: randomly dropping some units and their connections from dropout the network during training (Hinton et al. 2012 , Srivastava et al. 2014 . Tuning of hyperparameters is also important. The parameters of a neural network are the hyperparameter weights W and biases b; those are learned by gradient descent. The hyperparameters are things that are chosen by the algorithm designer; optimal values are tuned on a devset rather than by gradient descent learning on the training set. Hyperparameters include the learning rate \u03b7, the mini-batch size, the model architecture (the number of layers, the number of hidden nodes per layer, the choice of activation functions), how to regularize, and so on. Gradient descent itself also has many architectural variants such as Adam (Kingma and Ba, 2015) .", "Now that we've seen how to train a generic neural net, let's talk about the architecture for training a neural language model, setting the parameters \u03b8 = E, W, U, b.", "For some tasks, it's ok to freeze the embedding layer E with initial word2vec valfreeze ues. Freezing means we use word2vec or some other pretraining algorithm to compute the initial embedding matrix E, and then hold it constant while we only modify W, U, and b, i.e., we don't update E during language model training. However, often we'd like to learn the embeddings simultaneously with training the network. This is useful when the task the network is designed for (sentiment classification, or translation, or parsing) places strong constraints on what makes a good representation for words. Let's see how to train the entire model including E, i.e. to set all the parameters \u03b8 = E, W, U, b. We'll do this via gradient descent (Figure [ 5.5 ] ), using error backpropagation on the computation graph to compute the gradient. Training thus not only sets the weights W and U of the network, but also as we're predicting upcoming words, we're learning the embeddings E for each word that best predict upcoming words. Figure 7 .18 Learning all the way back to embeddings. Again, the embedding matrix E is shared among the 3 context words. Figure [ 7.18 ] shows the set up for a window size of N=3 context words. The input x consists of 3 one-hot vectors, fully connected to the embedding layer via 3 instanti-ations of the embedding matrix E. We don't want to learn separate weight matrices for mapping each of the 3 previous words to the projection layer. We want one single embedding dictionary E that's shared among these three. That's because over time, many different words will appear as w t\u22122 or w t\u22121 , and we'd like to just represent each word with one vector, whichever context position it appears in. Recall that the embedding weight matrix E has a column for each word, each a column vector of d dimensions, and hence has dimensionality d \u00d7 |V |.", "Generally training proceeds by taking as input a very long text, concatenating all the sentences, starting with random weights, and then iteratively moving through the text predicting each word w t . At each word w t , we use the cross-entropy (negative log likelihood) loss. Recall that the general form for this (repeated from Eq. [ 7.26 ] is:", "This gradient can be computed in any standard neural network framework which will then backpropagate through \u03b8 = E, W, U, b.", "English adpositions occur before nouns, hence are called prepositions. They can preposition indicate spatial or temporal relations, whether literal (on it, before then, by the house) or metaphorical (on time, with gusto, beside herself), and relations like marking the agent in Hamlet was written by Shakespeare.", "complementizer Pronouns act as a shorthand for referring to an entity or event. Personal propronoun nouns refer to persons or entities (you, she, I, it, me, etc.). Possessive pronouns are forms of personal pronouns that indicate either actual possession or more often just an abstract relation between the person and some object (my, your, his, her, its, one's, our, their). Wh-pronouns (what, who, whom, whoever) are used in certain wh question forms, or act as complementizers (Frida, who married Diego. . . ). Auxiliary verbs mark semantic features of a main verb such as its tense, whether auxiliary it is completed (aspect), whether it is negated (polarity), and whether an action is necessary, possible, suggested, or desired (mood). English auxiliaries include the copula verb be, the two verbs do and have, forms, as well as modal verbs used to copula modal mark the mood associated with the event depicted by the main verb: can indicates ability or possibility, may permission or possibility, must necessity. An English-specific tagset, the 45-tag Penn Treebank tagset (Marcus et al., 1993) , shown in Figure [ 8.2 ] , has been used to label many syntactically annotated corpora like the Penn Treebank corpora, so is worth knowing about. Below we show some examples with each word tagged according to both the UD and Penn tagsets. Notice that the Penn tagset distinguishes tense and participles on verbs, and has a special tag for the existential there construction in English. Note that since New England Journal of Medicine is a proper noun, both tagsets mark its component nouns as NNP, including journal and medicine, which might otherwise be labeled as common nouns (NOUN/NN).", "A named entity is, roughly speaking, anything that can be referred to with a proper name: a person, a location, an organization. The text contains 13 mentions of named entities including 5 organizations, 4 locations, 2 times, 1 person, and 1 mention of money. Figure 8 .5 shows typical generic named entity types. Many applications will also need to use specific entity types like proteins, genes, commercial products, or works of art.", "need to decide what's an entity and what isn't, and where the boundaries are. Indeed, most words in a text will not be named entities. Another difficulty is caused by type ambiguity. The mention JFK can refer to a person, the airport in New York, or any number of schools, bridges, and streets around the United States. Some examples of this kind of cross-type confusion are given in Figure 8 The standard approach to sequence labeling for a span-recognition problem like NER is BIO tagging (Ramshaw and Marcus, 1995) . This is a method that allows us to treat NER like a word-by-word sequence labeling task, via tags that capture both the boundary and the named entity type. Consider the following sentence: variants called IO tagging and BIOES tagging. In BIO tagging we label any token that begins a span of interest with the label B, tokens that occur inside a span are tagged with an I, and any tokens outside of any span of interest are labeled O. While there is only one O tag, we'll have distinct B and I tags for each named entity class. The number of tags is thus 2n + 1 tags, where n is the number of entity types. BIO tagging can represent exactly the same information as the bracketed notation, but has the advantage that we can represent the task in the same simple sequence modeling way as part-of-speech tagging: assigning a single label y i to each input word x i : We've also shown two variant tagging schemes: IO tagging, which loses some information by eliminating the B tag, and BIOES tagging, which adds an end tag E for the end of a span, and a span tag S for a span consisting of only one word. A sequence labeler (HMM, CRF, RNN, Transformer, etc.) is trained to label each token in a text with tags that indicate the presence (or absence) of particular kinds of named entities.", "The HMM is based on augmenting the Markov chain. A Markov chain is a model Markov chain that tells us something about the probabilities of sequences of random variables, states, each of which can take on values from some set. These sets can be words, or tags, or symbols representing anything, for example the weather. A Markov chain makes a very strong assumption that if we want to predict the future in the sequence, all that matters is the current state. All the states before the current state have no impact on the future except via the current state. It's as if to predict tomorrow's weather you could examine today's weather but you weren't allowed to look at yesterday's weather. More formally, consider a sequence of state variables q 1 , q 2 , ..., q i . A Markov model embodies the Markov assumption on the probabilities of this sequence: that", "Markov Assumption: Figure 8 .8a shows a Markov chain for assigning a probability to a sequence of weather events, for which the vocabulary consists of HOT, COLD, and WARM. The states are represented as nodes in the graph, and the transitions, with their probabilities, as edges. The transitions are probabilities: the values of arcs leaving a given state must sum to 1. Figure 8 .8b shows a Markov chain for assigning a probability to a sequence of words w 1 ...w t . This Markov chain should be familiar; in fact, it represents a bigram language model, with each edge expressing the probability p(w i |w j )! Given the two models in Figure [ 8.8 ] , we can assign a probability to any sequence from our vocabulary.", ". . a NN a transition probability matrix A, each a i j representing the probability of moving from state i to state j, s.t.", "([ 8.4 ]) hot hot hot hot ([ 8.5 ]) cold hot cold hot What does the difference in these probabilities tell you about a real-world weather fact encoded in Figure 8 .8a?", ". . a NN a transition probability matrix A, each a i j representing the probability of moving from state i to state j,", "The A matrix contains the tag transition probabilities P(t i |t i\u22121 ) which represent the probability of a tag occurring given the previous tag. For example, modal verbs like will are very likely to be followed by a verb in the base form, a VB, like race, so we expect this probability to be high. We compute the maximum likelihood estimate of this transition probability by counting, out of the times we see the first tag in a labeled corpus, how often the first tag is followed by the second:", "The A transition probabilities, and B observation likelihoods of the HMM are illustrated in Figure 8 .9 for three states in an HMM part-of-speech tagger; the full tagger would have one state for each tag.", "For any model, such as an HMM, that contains hidden variables, the task of determining the hidden variables sequence corresponding to the sequence of observations is called decoding. More formally, Figure 8 .9 An illustration of the two parts of an HMM representation: the A transition probabilities used to compute the prior probability, and the B observation likelihoods that are associated with each state, one likelihood for each possible observation word.", "For part-of-speech tagging, the goal of HMM decoding is to choose the tag sequence t 1 . . .t n that is most probable given the observation sequence of n words w 1 . . . w n :t", "Plugging the simplifying assumptions from Eq. [ 8.15 ] and Eq. [ 8.16 ] into Eq. [ 8.14 ] results in the following equation for the most probable tag sequence from a bigram tagger:t 1:n = argmax", "The Viterbi algorithm first sets up a probability matrix or lattice, with one column for each observation o t and one row for each state in the state graph. Each column thus has a cell for each state q i in the single combined automaton. Figure 8 .11 shows an intuition of this lattice for the sentence Janet will back the bill.", ". Like other dynamic programming algorithms, Viterbi fills each cell recursively. Given that we had already computed the probability of being in every state at time t \u2212 1, we compute the Viterbi probability by taking the most probable of the extensions of the paths that lead to the current cell. For a given state q j at time t, the value", "Let's tag the sentence Janet will back the bill; the goal is the correct series of tags (see also Figure [ 8.11 ] Let the HMM be defined by the two tables in Figure 8 .12 and Figure 8 .13. Figure 8 .12 lists the a i j probabilities for transitioning between the hidden states (part-of-speech tags). Figure 8 .13 expresses the b i (o t ) probabilities, the observation likelihoods of words given tags. This table is (slightly simplified) from counts in the WSJ corpus. So the word Janet only appears as an NNP, back has 4 possible parts of speech, and the word the can appear as a determiner or as an NNP (in titles like \"Somewhere Over the Rainbow\" all words are tagged as NNP). Figure 8 .14 The first few entries in the individual state columns for the Viterbi algorithm. Each cell keeps the probability of the best path so far and a pointer to the previous cell along that path. We have only filled out columns 1 and 2; to avoid clutter most cells with value 0 are left empty. The rest is left as an exercise for the reader. After the cells are filled in, backtracing from the end state, we should be able to reconstruct the correct state sequence NNP MD VB DT NN. Figure 8 .14 shows a fleshed-out version of the sketch we saw in Figure 8 .11, the Viterbi lattice for computing the best hidden state sequence for the observation sequence Janet will back the bill.", "There are N = 5 state columns. We begin in column 1 (for the word Janet) by setting the Viterbi value in each cell to the product of the \u03c0 transition probability (the start probability for that state i, which we get from the <s > entry of Figure [ 8.12 ]) , and the observation likelihood of the word Janet given the tag for that cell. Most of the cells in the column are zero since the word Janet cannot be any of those tags. The reader should find this in Figure [ 8.14 ]. Next, each cell in the will column gets updated. For each state, we compute the value viterbi[s,t] by taking the maximum over the extensions of all the paths from the previous column that lead to the current cell according to Eq. [ 8.19 ] . We have shown the values for the MD, VB, and NN cells. Each cell gets the max of the 7 values from the previous column, multiplied by the appropriate transition probability; as it happens in this case, most of them are zero from the previous column. The remaining value is multiplied by the relevant observation probability, and the (trivial) max is taken. In this case the final value, 2.772e-8, comes from the NNP state at the previous column. The reader should fill in the rest of the lattice in Figure 8 .14 and backtrace to see whether or not the Viterbi algorithm returns the gold state sequence NNP MD VB DT NN.", "Let's look at some of these features in detail, since the reason to use a discriminative sequence model is that it's easier to incorporate a lot of features. 2 Again, in a linear-chain CRF, each local feature f k at position i can depend on any information from: (y i\u22121 , y i , X, i). So some legal features representing common situations might be the following:", "Concretely, this involves filling an N \u00d7T array with the appropriate values, maintaining backpointers as we proceed. As with HMM Viterbi, when the table is filled, we simply follow pointers back from the maximum value in the final column to retrieve the desired set of labels.", "The requisite changes from HMM Viterbi have to do only with how we fill each cell. Recall from Eq. [ 8.19 ] that the recursive step of the Viterbi equation computes the Viterbi value of time t for state j as", "[ 1. ] First, use high-precision rules to tag unambiguous entity mentions. [ 2. ] Then, search for substring matches of the previously detected names. [ 3. ] Use application-specific name lists to find likely domain-specific mentions. [ 4. ] Finally, apply supervised sequence labeling techniques that use tags from previous stages as additional features.", "These productive word-formation processes result in a large vocabulary for these languages: a 250,000 word token corpus of Hungarian has more than twice as many word types as a similarly sized corpus of English (Oravecz and Dienes, 2002) , while a 10 million word token corpus of Turkish contains four times as many word types as a similarly sized English corpus (Hakkani-T\u00fcr et al., 2002) . Large vocabularies mean many unknown words, and these unknown words cause significant performance degradations in a wide variety of languages (including Czech, Slovene, Estonian, and Romanian) (Haji\u010d, 2000) .", "Highly inflectional languages also have much more information than English coded in word morphology, like case (nominative, accusative, genitive) or gender (masculine, feminine). Because this information is important for tasks like parsing and coreference resolution, part-of-speech taggers for morphologically rich languages need to label words with case and gender information. Tagsets for morphologically rich languages are therefore sequences of morphological tags rather than a single primitive tag. Here's a Turkish example, in which the word izin has three possible morphological/part-of-speech tags and meanings (Hakkani-T\u00fcr et al., 2002) :", "Log-linear models for POS tagging were introduced by Ratnaparkhi (1996), who introduced a system called MXPOST which implemented a maximum entropy Markov model (MEMM), a slightly simpler version of a CRF. Around the same time, sequence labelers were applied to the task of named entity tagging, first with HMMs (Bikel et al., 1997) and MEMMs (McCallum et al., 2000) , and then once CRFs were developed (Lafferty et al. 2001) , they were also applied to NER (Mc-Callum and Li, 2003) . A wide exploration of features followed (Zhou et al., 2005) . Neural approaches to NER mainly follow from the pioneering results of Collobert et al. 2011, who applied a CRF on top of a convolutional net. BiLSTMs with word and character-based embeddings as input followed shortly and became a standard neural algorithm for NER (Huang et al. 2015 , Ma and Hovy 2016 , Lample et al. 2016 followed by the more recent use of Transformers and BERT.", "Supervised tagging relies heavily on in-domain training data hand-labeled by experts. Ways to relax this assumption include unsupervised algorithms for clustering words into part-of-speech-like classes, summarized in Christodoulopoulos et al. (2010), and ways to combine labeled and unlabeled data, for example by co-training (Clark et al. 2003; S\u00f8gaard 2010) .", "The feedforward networks of Chapter 7 also assumed simultaneous access, although they also had a simple model for time. Recall that we applied feedforward networks to language modeling by having them look only at a fixed-size window of words, and then sliding this window over the input, making independent predictions along the way. Figure 9 .1, reproduced from Chapter 7, shows a neural language model with window size 3 predicting what word follows the input for all the. Subsequent words are predicted by sliding the window forward a word at a time.", "The simple feedforward sliding-window is promising, but isn't a completely satisfactory solution to temporality. By using embeddings as inputs, it does solve the main problem of the simple n-gram models of Chapter 3 (recall that n-grams were based on words rather than embeddings, making them too literal, unable to generalize across contexts of similar words). But feedforward networks still share another weakness of n-gram approaches: limited context. Anything outside the context window has no impact on the decision being made. Yet many language tasks require access to information that can be arbitrarily distant from the current word. Second, the use of windows makes it difficult for networks to learn systematic patterns arising from phenomena like constituency and compositionality: the way the meaning of words in phrases combine together. For example, in Figure 9 .1 the phrase all the appears in one window in the second and third positions, and in the next window in the first and second positions, forcing the network to learn two separate patterns for what should be the same item.", "We instantiate this intuition by using perplexity to measure the quality of a perplexity language model. Recall from page 36 that the perplexity (PP) of a model \u03b8 on an unseen test set is the inverse probability that \u03b8 assigns to the test set, normalized by the test set length. For a test set w 1:n , the perplexity is", "x t h t y t Figure 9 .2 Simple recurrent neural network after Elman (1990). The hidden layer includes a recurrent connection as part of its input. That is, the activation value of the hidden layer depends on the current input as well as the activation value of the hidden layer from the previous time step. Figure 9 .2 illustrates the structure of an RNN. As with ordinary feedforward networks, an input vector representing the current input, x t , is multiplied by a weight matrix and then passed through a non-linear activation function to compute the values for a layer of hidden units. This hidden layer is then used to calculate a corresponding output, y t . In a departure from our earlier window-based approach, sequences are processed by presenting one item at a time to the network. We'll use [ 9.2 ] [ \u2022 ] RECURRENT NEURAL NETWORKS 181 subscripts to represent time, thus x t will mean the input vector x at time t. The key difference from a feedforward network lies in the recurrent link shown in the figure with the dashed line. This link augments the input to the computation at the hidden layer with the value of the hidden layer from the preceding point in time.", "Adding this temporal dimension makes RNNs appear to be more complex than non-recurrent architectures. But in reality, they're not all that different. Given an input vector and the values for the hidden layer from the previous time step, we're still performing the standard feedforward calculation introduced in Chapter 7. To see this, consider Figure 9 .3 which clarifies the nature of the recurrence and how it factors into the computation at the hidden layer. The most significant change lies in the new set of weights, U, that connect the hidden layer from the previous time step to the current hidden layer. These weights determine how the network makes use of past context in calculating the output for the current input. As with the other weights in the network, these connections are trained via backpropagation. Figure 9 .3 Simple recurrent neural network illustrated as a feedforward network.", "As with feedforward networks, we'll use a training set, a loss function, and backpropagation to obtain the gradients needed to adjust the weights in these recurrent networks. As shown in Figure 9 .3, we now have 3 sets of weights to update: W, the weights from the input layer to the hidden layer, U, the weights from the previous hidden layer to the current hidden layer, and finally V, the weights from the hidden layer to the output layer. Figure 9 .5 highlights two considerations that we didn't have to worry about with backpropagation in feedforward networks. First, to compute the loss function for the output at time t we need the hidden layer from time t \u2212 1. Second, the hidden layer at time t influences both the output at time t and the hidden layer at time t + 1 (and hence the output and loss at t + 1). It follows from this that to assess the error accruing to h t , we'll need to know its influence on both the current output as well as the ones that follow.", "For applications that involve much longer input sequences, such as speech recognition, character-level processing, or streaming of continuous inputs, unrolling an entire input sequence may not be feasible. In these cases, we can unroll the input into manageable fixed-length segments and treat each segment as a distinct training item.", "The probability of an entire sequence is just the product of the probabilities of each item in the sequence, where we'll use y i [w i ] to mean the probability of the true word w i at time step i.", "The weights in the network are adjusted to minimize the average CE loss over the training sequence via gradient descent. Figure 9 .6 illustrates this training regimen.", "Careful readers may have noticed that the input embedding matrix E and the final layer matrix V, which feeds the output softmax, are quite similar. The rows of E represent the word embeddings for each word in the vocabulary learned during the training process with the goal that words that have similar meaning and function will have similar embeddings. And, since the length of these embeddings corresponds to the size of the hidden layer d h , the shape of the embedding matrix E is |V | \u00d7 d h .", "In sequence labeling, the network's task is to assign a label chosen from a small fixed set of labels to each element of a sequence, like the part-of-speech tagging and named entity recognition tasks from Chapter 8. In an RNN approach to sequence labeling, inputs are word embeddings and the outputs are tag probabilities generated by a softmax layer over the given tagset, as illustrated in Figure 9 Figure 9 .7 Part-of-speech tagging as sequence labeling with a simple RNN. Pre-trained word embeddings serve as inputs and a softmax layer provides a probability distribution over the part-of-speech tags as output at each time step.", "To apply RNNs in this setting, we pass the text to be classified through the RNN a word at a time generating a new hidden layer at each time step. We can then take the hidden layer for the last token of the text, h n , to constitute a compressed representation of the entire sequence. We can pass this representation h n to a feedforward network that chooses a class via a softmax over the possible classes. Figure 9 .8 illustrates this approach.", "RNN-based language models can also be used to generate text. Text generation is of enormous practical importance, part of tasks like question answering, machine translation, text summarization, and conversational dialogue; any ask where a system needs to produce text, conditioned on some other text. Recall back in Chapter 3 we saw how to generate text from an n-gram language model by adapting a technique suggested contemporaneously by Claude Shannon (Shannon, 1951) and the psychologists George Miller and Selfridge (Miller and Selfridge, 1950) . We first randomly sample a word to begin a sequence based on its suitability as the start of a sequence. We then continue to sample words conditioned on our previous choices until we reach a pre-determined length, or an end of sequence token is generated.", "[ \u2022 ] Sample a word in the output from the softmax distribution that results from using the beginning of sentence marker, <s>, as the first input. [ \u2022 ] Use the word embedding for that first word as the input to the network at the next time step, and then sample the next word in the same fashion. [ \u2022 ] Continue generating until the end of sentence marker, </s>, is sampled or a fixed length limit is reached.", "Technically an autoregressive model is a model that predicts a value at time t based on a linear function of the previous values at times t \u2212 1, t \u2212 2, and so on. Although language models are not linear (since they have many layers of non-linearities), we loosely refer to this generation technique as autoregressive generation since the word generated at each time step is conditioned on the word selected by the network from the previous step. Figure 9 .9 illustrates this approach. In this figure, the details of the RNN's hidden layers and recurrent connections are hidden within the blue block. This simple architecture underlies state-of-the-art approaches to applications such as machine translation, summarization, and question answering. The key to these approaches is to prime the generation component with an appropriate context. That is, instead of simply using <s> to get things started we can provide a richer task-appropriate context; for translation the context is the sentence in the source language; for summarization it's the long text we want to summarize. We'll discuss the application of contextual generation to the problem of summarization in Section [ 9.9 ] in the context of transformer-based language models, and then again in Chapter 10 when we introduce encoder-decoder models.", "15) Figure 9 .11 illustrates such a bidirectional network that concatenates the outputs of the forward and backward pass. Other simple ways to combine the forward and backward contexts include element-wise addition or multiplication. The output at each step in time thus captures information to the left and to the right of the current input. In sequence labeling applications, these concatenated outputs can serve as the basis for a local labeling decision.", "([ 9.26 ]) Figure 9 .13 illustrates the complete computation for a single LSTM unit. Given the appropriate weights for the various gates, an LSTM accepts as input the context layer, and hidden layer from the previous time step, along with the current input vector. It then generates updated context and hidden vectors as output. The hidden layer, h t , can be used as input to subsequent layers in a stacked RNN, or to generate an output for the final layer of a network.", "The neural units used in LSTMs are obviously much more complex than those used in basic feedforward networks. Fortunately, this complexity is encapsulated within the basic processing units, allowing us to maintain modularity and to easily experiment with different architectures. To see this, consider Figure 9 .14 which illustrates the inputs and outputs associated with each kind of unit. At the far left, (a) is the basic feedforward unit where a single set of weights and a single activation function determine its output, and when arranged in a layer there are no connections among the units in the layer. Next, (b) represents the unit in a simple recurrent network. Now there are two inputs and an additional set of weights to go with it. However, there is still a single activation function and output.", "While the addition of gates allows LSTMs to handle more distant information than RNNs, they don't completely solve the underlying problem: passing information through an extended series of recurrent connections leads to information loss and difficulties in training. Moreover, the inherently sequential nature of recurrent networks makes it hard to do computation in parallel. These considerations led to the development of transformers -an approach to sequence processing that eliminates transformers recurrent connections and returns to architectures reminiscent of the fully connected networks described earlier in Chapter 7. Transformers map sequences of input vectors (x 1 , ..., x n ) to sequences of output vectors (y 1 , ..., y n ) of the same length. Transformers are made up of stacks of transformer blocks, which are multilayer networks made by combining simple linear layers, feedforward networks, and self-attention layers, they key innovation of self-attention transformers. Self-attention allows a network to directly extract and use information from arbitrarily large contexts without the need to pass it through intermediate recurrent connections as in RNNs. We'll start by describing how self-attention works and then return to how it fits into larger transformer blocks. Figure 9 .15 illustrates the flow of information in a single causal, or backward looking, self-attention layer. As with the overall transformer, a self-attention layer maps input sequences (x 1 , ..., x n ) to output sequences of the same length (y 1 , ..., y n ). When processing each item in the input, the model has access to all of the inputs up to and including the one under consideration, but no access to information about inputs beyond the current one. In addition, the computation performed for each item is independent of all the other computations. The first point ensures that we can use this approach to create language models and use them for autoregressive generation, and the second point means that we can easily parallelize both forward inference and training of such models.", "Figure 9 .15 Information flow in a causal (or masked) self-attention model. In processing each element of the sequence, the model attends to all the inputs up to, and including, the current one. Unlike RNNs, the computations at each time step are independent of all the other steps and therefore can be performed in parallel.", "To capture these three different roles, transformers introduce weight matrices W Q , W K , and W V . These weights will be used to project each input vector x i into a representation of its role as a key, query, or value.", "Given these projections, the score between a current focus of attention, x i and an element in the preceding context, x j consists of a dot product between its query vector q i and the preceding element's key vectors k j . This dot product has the right shape since both the query and the key are of dimensionality 1 \u00d7 d. Let's update our previous comparison calculation to reflect this, replacing Eq. [ 9.27 ] with Eq. [ 9.32 ]:", "The result of a dot product can be an arbitrarily large (positive or negative) value. Exponentiating such large values can lead to numerical issues and to an effective loss of gradients during training. To avoid this, the dot product needs to be scaled in a suitable fashion. A scaled dot-product approach divides the result of the dot product by a factor related to the size of the embeddings before passing them through the softmax. A typical approach is to divide the dot product by the square root of the dimensionality of the query and key vectors (d k ), leading us to update our scoring function one more time, replacing Eq. [ 9.27 ] and Eq. [ 9.32 ] with Eq. [ 9.34 ]:", "This description of the self-attention process has been from the perspective of computing a single output at a single time step i. However, since each output, y i , is computed independently this entire process can be parallelized by taking advantage of efficient matrix multiplication routines by packing the input embeddings of the N tokens of the input sequence into a single matrix X \u2208 R N\u00d7d . That is, each row of X is the embedding of one token of the input. We then multiply X by the key, query, and value matrices (all of dimensionality d \u00d7 d) to produce matrices Q \u2208 R N\u00d7d , K \u2208 R N\u00d7d , and V \u2208 R N\u00d7d , containing all the key, query, and value vectors:", "Unfortunately, this process goes a bit too far since the calculation of the comparisons in QK results in a score for each query value to every key value, including those that follow the query. This is inappropriate in the setting of language modeling since guessing the next word is pretty simple if you already know it. To fix this, the elements in the upper-triangular portion of the matrix are zeroed out (set to \u2212\u221e), thus eliminating any knowledge of words that follow in the sequence. Figure 9 .17 depicts the QK matrix. (we'll see in Chapter 11 how to make use of words in the future for tasks that need it).", "The self-attention calculation lies at the core of what's called a transformer block, which, in addition to the self-attention layer, includes additional feedforward layers, residual connections, and normalizing layers. The input and output dimensions of these blocks are matched so they can be stacked just as was the case for stacked RNNs. Figure 9 .18 illustrates a standard transformer block consisting of a single attention layer followed by a fully-connected feedforward layer with residual connections and layer normalizations following each. We've already seen feedforward layers in Chapter 7, but what are residual connections and layer norm? In deep networks, residual connections are connections that pass information from a lower layer to a higher layer without going through the intermediate layer. Allowing information from the activation going forward and the gradient going backwards to skip a layer improves learning and gives higher level layers direct access to information from lower layers (He et al., 2016) . Residual connections in transformers are implemented by added a layer's input vector to its output vector before passing it forward . In the transformer block shown in Figure 9 .18, residual connections are used with both the attention and feedforward sublayers. These summed vectors are then normalized using layer normalization (Ba et al., 2016 ). If we think of a layer as one long vector of units, the resulting function computed in a transformer block can be expressed as:", "Layer normalization (or layer norm) is one of many forms of normalization that layer norm can be used to improve training performance in deep neural networks by keeping the values of a hidden layer in a range that facilitates gradient-based training. Layer norm is a variation of the standard score, or z-score, from statistics applied to a single hidden layer. The first step in layer normalization is to calculate the mean, \u00b5, and standard deviation, \u03c3 , over the elements of the vector to be normalized. Given a hidden layer with dimensionality d h , these values are calculated as follows.", "Finally, in the standard implementation of layer normalization, two learnable parameters, \u03b3 and \u03b2 , representing gain and offset values, are introduced.", "i . These are used to project the inputs into separate key, value, and query embeddings separately for each head, with the rest of the self-attention computation remaining unchanged. In multi-head attention, instead of using the model dimension d that's used for the input and output from the model, the key and query embeddings have dimensionality d k , and the value embeddings are dimensionality d v (in the original transformer paper d k = d v = 64). Thus for each head i, we have weight layers W Q i \u2208 R d\u00d7d k , W K i \u2208 R d\u00d7d k , and", ", and these get multiplied by the inputs packed into X to produce Q \u2208 R N\u00d7d k , K \u2208 R N\u00d7d k , and V \u2208 R N\u00d7d v . The output of each of the h heads is of shape N \u00d7 d v , and so the output of the multi-head layer with h heads consists of h vectors of shape N \u00d7 d v . To make use of these vectors in further processing, they are combined and then reduced down to the original input dimension d. This is accomplished by concatenating the outputs from each head and then using yet another linear projection, W O \u2208 R hd v \u00d7d , to reduce it to the original output dimension for each token, or a total N \u00d7 d output. Figure 9 .19 illustrates this approach with 4 self-attention heads. This multihead layer replaces the single self-attention layer in the transformer block shown earlier in Figure 9 .18, the rest of the transformer block with its feedforward layer, residual connections, and layer norms remains the same.", "How does a transformer model the position of each token in the input sequence? With RNNs, information about the order of the inputs was built into the structure of the model. Unfortunately, the same isn't true for transformers; the models as we've described them so far don't have any notion of the relative, or absolute, positions of the tokens in the input. This can be seen from the fact that if you scramble the order of the inputs in the attention computation in Figure 9 .16 you get exactly the same answer.", "Where do we get these positional embeddings? The simplest method is to start with randomly initialized embeddings corresponding to each possible input position up to some maximum length. For example, just as we have an embedding for the word fish, we'll have an embedding for the position 3. As with word embeddings, these positional embeddings are learned along with other parameters during training. To produce an input embedding that captures positional information, we just add the [ 9.7 ] [ \u2022 ] SELF-ATTENTION NETWORKS: TRANSFORMERS 201", "word embedding for each input to its corresponding positional embedding. This new embedding serves as the input for further processing. Figure 9 .20 shows the idea. A potential problem with the simple absolute position embedding approach is that there will be plenty of training examples for the initial positions in our inputs and correspondingly fewer at the outer length limits. These latter embeddings may be poorly trained and may not generalize well during testing. An alternative approach to positional embeddings is to choose a static function that maps integer inputs to realvalued vectors in a way that captures the inherent relationships among the positions. That is, it captures the fact that position 4 in an input is more closely related to position 5 than it is to position 17. A combination of sine and cosine functions with differing frequencies was used in the original transformer work. Developing better position representations is an ongoing research topic.", "Now that we've seen all the major components of transformers, let's examine how to deploy them as language models via semi-supervised learning. To do this, we'll proceed just as we did with the RNN-based approach: given a training corpus of plain text we'll train a model to predict the next word in a sequence using teacher forcing. Figure 9 .21 illustrates the general approach. At each step, given all the preceding words, the final transformer layer produces an output distribution over the entire vocabulary. During training, the probability assigned to the correct word is used to calculate the cross-entropy loss for each item in the sequence. As with RNNs, the loss for a training sequence is the average cross-entropy loss over the entire sequence.", "Linear Layer Figure 9 .21 Training a transformer as a language model. Note the key difference between this figure and the earlier RNN-based version shown in Figure 9 .6. There the calculation of the outputs and the losses at each step was inherently serial given the recurrence in the calculation of the hidden states. With transformers, each training item can be processed in parallel since the output for each element in the sequence is computed separately. Once trained, we can compute the perplexity of the resulting model, or autoregressively generate novel text just as with RNN-based models.", "Note the many ways the English and Chinese differ. For example the ordering differs in major ways; the Chinese order of the noun phrase is \"peaceful using outer space conference of suggestions\" while the English has \"suggestions of the ... conference on peaceful use of outer space\"). And the order differs in minor ways (the date is ordered differently). English requires the in many places that Chinese doesn't, and adds some details (like \"in which\" and \"it\") that aren't necessary in Chinese. Chinese doesn't grammatically mark plurality on nouns (unlike English, which has the \"-s\" in \"recommendations\"), and so the Chinese must use the modifier \u5404\u9879/various to make it clear that there is not just one recommendation. English capitalizes some words but not others.", "We'll introduce the algorithm in sections Section [ 10.2 ], and in following sections give important components of the model like beam search decoding, and we'll discuss how MT is evaluated, introducing the simple chrF metric.", "The way that languages differ in lexically dividing up conceptual space may be more complex than this one-to-many translation problem, leading to many-to-many mappings. For example, Figure 10 .2 summarizes some of the complexities discussed by Hutchins and Somers (1992) in translating English leg, foot, and paw, to French. For example, when leg is used about an animal it's translated as French jambe; but about the leg of a journey, as French etape; if the leg is of a chair, we use French pied.", "Further, one language may have a lexical gap, where no word or phrase, short of an explanatory footnote, can express the exact meaning of a word in the other language. For example, English does not have a word that corresponds neatly to Mandarin xi\u00e0o or Japanese oyak\u014dk\u014do (in English one has to make do with awkward phrases like filial piety or loving child, or good son/daughter for both). Finally, languages differ systematically in how the conceptual properties of an event are mapped onto specific words. Talmy (1985, 1991) noted that languages can be characterized by whether direction of motion and manner of motion are marked on the verb or on the \"satellites\": particles, prepositional phrases, or adverbial phrases. For example, a bottle floating out of a cave would be described in English with the direction marked on the particle out, while in Spanish the direction Verb-framed languages mark the direction of motion on the verb (leaving the verb-framed satellites to mark the manner of motion), like Spanish acercarse 'approach', alcanzar 'reach', entrar 'enter', salir 'exit'. Satellite-framed languages mark the satellite-framed direction of motion on the satellite (leaving the verb to mark the manner of motion), like English crawl out, float off, jump down, run after. Languages like Japanese, Tamil, and the many languages in the Romance, Semitic, and Mayan languages families, are verb-framed; Chinese as well as non-Romance Indo-European languages like English, Swedish, Russian, Hindi, and Farsi are satellite framed (Talmy 1991 , Slobin 1996 .", "Languages that can omit pronouns are called pro-drop languages. Even among the pro-drop languages, there are marked differences in frequencies of omission. Japanese and Chinese, for example, tend to omit far more than does Spanish. This dimension of variation across languages is called the dimension of referential density. We say that languages that tend to use more pronouns are more referentially referential density dense than those that use more zeros. Referentially sparse languages, like Chinese or Japanese, that require the hearer to do more inferential work to recover antecedents are also called cold languages. Languages that are more explicit and make it easier Marshall McLuhan's 1964 distinction between hot media like movies, which fill in many details for the viewer, versus cold media like comics, which require the reader to do more inferential work to fill out the representation (Bickel, 2003) .", "[ 1. ] An encoder that accepts an input sequence, x n 1 , and generates a corresponding sequence of contextualized representations, h n 1 . LSTMs, GRUs, convolutional networks, and Transformers can all be employed as encoders. [ 2. ] A context vector, c, which is a function of h n 1 , and conveys the essence of the input to the decoder.", "We only have to make one slight change to turn this language model with autoregressive generation into a translation model that can translate from a source text source in one language to a target text in a second: add an sentence separation marker at target the end of the source text, and then simply concatenate the target text. We briefly introduced this idea of a sentence separator token in Chapter 9 when we considered using a Transformer language model to do summarization, by training a conditional language model. If we call the source text x and the target text y, we are computing the probability p(y|x) as follows: Figure [ 10.4 ] shows the setup for a simplified version of the encoder-decoder model (we'll see the full model, which requires attention, in the next section). Figure [ 10.4 ] shows an English source text (\"the green witch arrived\"), a sentence separator token (<s>, and a Spanish target text (\"lleg\u00f3 la bruja verde\"). To translate a source text, we run it through the network performing forward inference to generate hidden states until we get to the end of the source. Then we begin autoregressive generation, asking for a word in the context of the hidden layer from the end of the source input as well as the end-of-sentence marker. Subsequent words are conditioned on the previous hidden state and the embedding for the last word generated. Let's formalize and generalize this model a bit in Figure 10 .5. (To help keep things straight, we'll use the superscripts e and d where needed to distinguish the hidden states of the encoder and the decoder.) The elements of the network on the left process the input sequence x and comprise the encoder. While our simplified figure shows only a single network layer for the encoder, stacked architectures are the norm, where the output states from the top layer of the stack are taken as the final representation. A widely used encoder design makes use of stacked biLSTMs where the hidden states from top layers from the forward and backward passes are concatenated as described in Chapter 9 to provide the contextualized representations for each time step. The entire purpose of the encoder is to generate a contextualized representation of the input. This representation is embodied in the final hidden state of the encoder, h e n . This representation, also called c for context, is then passed to the decoder. The decoder network on the right takes this state and uses it to initialize the first", "Note the differences between training (Figure [ 10.7 ] ) and inference ( Figure [ 10.4 ] ) with respect to the outputs at each time step. The decoder during inference uses its own estimated output\u0177 t as the input for the next time step x t+1 . Thus the decoder will tend to deviate more and more from the gold target sentence as it keeps generating more tokens. In training, therefore, it is more common to use teacher forcing in the teacher forcing decoder. Teacher forcing means that we force the system to use the gold target token from training as the next input x t+1 , rather than allowing it to rely on the (possibly erroneous) decoder output\u0177 t . This speeds up training.", "In the attention mechanism, as in the vanilla encoder-decoder model, the context vector c is a single vector that is a function of the hidden states of the encoder, that is, c = f (h e 1 . . . h e n ). Because the number of hidden states varies with the size of the", "The simplest such score, called dot-product attention, implements relevance as dot-product attention similarity: measuring how similar the decoder hidden state is to an encoder hidden state, by computing the dot product between them:", "To make use of these scores, we'll normalize them with a softmax to create a vector of weights, \u03b1 i j , that tells us the proportional relevance of each encoder hidden state j to the prior hidden decoder state, h d i\u22121 .", "Choosing the single most probable token to generate at each step is called greedy decoding; a greedy algorithm is one that make a choice that is locally optimal, whether or not it will turn out to have been the best choice with hindsight. Indeed, greedy search is not optimal, and may not find the highest probability translation. The problem is that the token that looks good to the decoder now might turn out later to have been the wrong choice! Let's see this by looking at the search tree, a graphical representation of the search tree choices the decoder makes in searching for the best translation, in which we view the decoding problem as a heuristic state-space search and systematically explore the space of possible outputs. In such a search tree, the branches are the actions, in this case the action of generating a token, and the nodes are the states, in this case the state of having generated a particular prefix. We are searching for the best action sequence, i.e. the target string with the highest probability. Figure 10 .11 demonstrates the problem, using a made-up example. Notice that the most probable sequence is ok ok </s> (with a probability of .4*.7*[ 1.0 ]), but a greedy search algorithm will fail to find it, because it incorrectly chooses yes as the first word since it has the highest local probability. Figure 10 .11 A search tree for generating the target string T = t 1 ,t 2 , ... from the vocabulary V = {yes, ok, <s>}, given the source string, showing the probability of generating each token from that state. Greedy search would choose yes at the first time step followed by yes, instead of the globally most probable sequence ok ok.", "At subsequent steps, each of the k best hypotheses is extended incrementally by being passed to distinct decoders, which each generate a softmax over the entire vocabulary to extend the hypothesis to every possible next token. Each of these k * V hypotheses is scored by P(y i |x, y <i ): the product of the probability of current word choice multiplied by the probability of the path that led to it. We then prune the k * V hypotheses down to the k best hypotheses, so there are never more than k hypotheses [ 10.5 ] [ \u2022 ] BEAM SEARCH 221 at the frontier of the search, and never more than k decoders. Figure 10 .12 illustrates this process with a beam width of 2. This process continues until a </s> is generated indicating that a complete candidate output has been found. At this point, the completed hypothesis is removed from the frontier and the size of the beam is reduced by one. The search continues until the beam has been reduced to 0. The result will be k hypotheses.", "Let's see how the scoring works in detail, scoring each node by its log probability. Recall from Eq. [ 10.10 ] that we can use the chain rule of probability to break down p(y|x) into the product of the probability of each word given its prior context, which we can turn into a sum of logs (for an output string of length t):", "score(y) = log P(y|x) = log (P(y 1 |x)P(y 2 |y 1 , x)P(y 3 |y 1 , y 2 , x)...P(y t |y 1 , ..., y t\u22121 , x)) = t i=1 log P(y i |y 1 , ..., y i\u22121 , x) ([ 10.19 ]) Thus at each step, to compute the probability of a partial translation, we simply add the log probability of the prefix translation so far to the log probability of generating the next token. Figure 10 .13 shows the scoring for the example sentence shown in Figure 10 .12, using some simple made-up probabilities. Log probabilities are negative or 0, and the max of two log probabilities is the one that is greater (closer to 0). Figure 10 .13 Scoring for beam search decoding with a beam width of k = 2. We maintain the log probability of each hypothesis in the beam by incrementally adding the logprob of generating each next token. Only the top k paths are extended to the next step. Figure 10 .14 gives the algorithm. One problem arises from the fact that the completed hypotheses may have different lengths. Because models generally assign lower probabilities to longer strings, a naive algorithm would also choose shorter strings for y. This was not an issue during the earlier steps of decoding; due to the breadth-first nature of beam search all the hypotheses being compared had the same length. The usual solution to this is function BEAMDECODE(c, beam width) returns best paths y 0 , h 0 \u2190 0 path \u2190 () complete paths \u2190 () state \u2190 (c, y 0 , h 0 , path)", "The encoder-decoder architecture can also be implemented using transformers (rather than RNN/LSTMs) as the component modules. At a high-level, the architecture, sketched in Figure 10 .15, is quite similar to what we saw for RNNs. It consists of an encoder that takes the source language input words X = x 1 , ..., x T and maps them to an output representation H enc = h 1 , ..., h T ; usually via N = 6 stacked encoder blocks. The decoder, just like the encoder-decoder RNN, is essentially a conditional language model that attends to the encoder representation and generates the target words one by one, at each timestep conditioning on the source sentence and the previously generated target language words. Figure 10 .15 The encoder-decoder architecture using transformer components. The encoder uses the transformer blocks we saw in Chapter 9, while the decider uses a more powerful block with an extra encoder-decoder attention layer. The final output of the encoder H enc = h 1 , ..., h T is used to form the K and V inputs to the crossattention layer in each decoder block.", "[ 1. ] Initialize the wordpiece lexicon with characters (for example a subset of Unicode characters, collapsing all the remaining characters to a special unknown character token). [ 2. ] Repeat until there are V wordpieces:", "Standard training corpora for MT come as aligned pairs of sentences. When creating new corpora, for example for underresourced languages or new domains, these sentence alignments must be created. Figure 10 .17 gives a sample hypothetical sentence alignment. Given two documents that are translations of each other, we generally need two steps to produce sentence alignments:", "Using humans to evaluate is most accurate, but automatic metrics are also used for convenience.", "Given the hypothesis and the reference, chrF is given a parameter k indicating the length of character n-grams to be considered, and computes the average of the k precisions (unigram precision, bigram, and so on) and the average of the k recalls (unigram recall, bigram recall, etc. ):", "Let's see how we computed that chrF value for HYP1 (we'll leave the computation of the chrF value for HYP2 as an exercise for the reader). First, chrF ignores spaces, so we'll remove them from both the reference and hypothesis: REF: witnessforthepast, (18 unigrams, 17 bigrams) HYP1: witnessofthepast, (17 unigrams, 16 bigrams)", "For example, in some situations we might have datasets that have human assessments of translation quality. Such datasets consists of tuples (x,x, r), where x = (x 1 , . . . , x n ) is a reference translation,x = (x 1 , . . . ,x m ) is a candidate machine translation, and r \u2208 R is a human rating that expresses the quality ofx with respect to x. Given such data, algorithms like COMET (Rei et al., 2020) BLEURT (Sellam et al., 2020) train a predictor on the human-labeled datasets, for example by passing x andx through a version of BERT (trained with extra pretraining, and then finetuned on the human-labeled sentences), followed by a linear layer that is trained to predict r. The output of such models correlates highly with human labels.", "Machine translation raises many of the same ethical issues that we've discussed in earlier chapters. For example, consider MT systems translating from Hungarian (which has the gender neutral pronoun\u0151) or Spanish (which often drops pronouns) into English (in which pronouns are obligatory, and they have grammatical gender). When translating a reference to a person described without specified gender, MT systems often default to male gender (Schiebinger 2014 , Prates et al. 2019 . And MT systems often assign gender according to culture stereotypes of the sort we saw in Section [ 6.11 ]. Figure 10 .19 shows examples from Prates et al. 2019, in which Hungarian gender-neutral\u0151 is a nurse is translated with she, but gender-neutral\u0151 is a CEO is translated with he. Prates et al. (2019) find that these stereotypes can't completely be accounted for by gender bias in US labor statistics, because the biases are amplified by MT systems, with pronouns being mapped to male or female gender with a probability higher than if the mapping was based on actual labor employment statistics.", "Another is the need for low-resource algorithms that can translate to and from all the world's languages, the vast majority of which do not have large parallel training texts available. This problem is exacerbated by the tendency of many MT approaches to focus on the case where one of the languages is English (Anastasopoulos and Neubig, 2020). \u2200 et al. (2020) propose a participatory design process to encourage content creators, curators, and language technologists who speak these low-resourced languages to participate in developing MT algorithms. They pro-low-resourced languages vide online groups, mentoring, and infrastructure, and report on a case study on developing MT algorithms for low-resource African languages.", "[ \u2022 ] Languages have divergences, both structural and lexical, that make translation difficult. [ \u2022 ] The linguistic field of typology investigates some of these differences; languages can be classified by their position along typological dimensions like whether verbs precede their objects. [ \u2022 ] Encoder-decoder networks (either for RNNs or transformers) are composed of an encoder network that takes an input sequence and creates a contextualized representation of it, the context. This context representation is then passed to a decoder which generates a task-specific output sequence. [ \u2022 ] The attention mechanism in RNNs, and cross-attention in transformers, allows the decoder to view information from all the hidden states of the encoder. [ \u2022 ] For the decoder, choosing the single most probable token to generate at each step is called greedy decoding. [ \u2022 ] In beam search, instead of choosing the best token to generate at each timestep, we keep k possible tokens at each step. This fixed-size memory footprint k is called the beam width. [ \u2022 ] Machine translation models are trained on a parallel corpus, sometimes called a bitext, a text that appears in two (or more) languages. [ \u2022 ] Backtranslation is a way of making use of monolingual corpora in the target language by running a pilot MT engine backwards to create synthetic bitexts. [ \u2022 ] MT is evaluated by measuring a translation's adequacy (how well it captures the meaning of the source sentence) and fluency (how fluent or natural it is in the target language). Human evaluation is the gold standard, but automatic evaluation metrics like chrF, which measure character n-gram overlap with human translations, or more recent metrics based on embedding similarity, are also commonly used.", "As high-quality MT proved elusive (Bar-Hillel, 1960) , there grew a consensus on the need for better evaluation and more basic research in the new fields of formal and computational linguistics. This consensus culminated in the famously critical ALPAC (Automatic Language Processing Advisory Committee) report of 1966 of (Pierce et al., 1966 that led in the mid 1960s to a dramatic cut in funding for MT in the US. As MT research lost academic respectability, the Association for Machine Translation and Computational Linguistics dropped MT from its name. Some MT developers, however, persevered, and there were early MT systems like M\u00e9t\u00e9o, which translated weather forecasts from English to French (Chandioux, 1976) , and industrial systems like Systran.", "\"How much do we know at any time? Much more, or so I believe, than we know we know.\" Agatha Christie, The Moving Finger", "A simple consequence of these facts is that children have to learn about 7 to 10 words a day, every single day, to arrive at observed vocabulary levels by the time they are 20 years of age. And indeed empirical estimates of vocabulary growth in late elementary through high school are consistent with this rate. How do children achieve this rate of vocabulary growth given their daily experiences during this period? We know that most of this growth is not happening through direct vocabulary instruction in school since these methods are largely ineffective, and are not deployed at a rate that would result in the reliable acquisition of words at the required rate.", "Let's begin by introducing the bidirectional transformer encoder that underlies models like BERT and its descendants like RoBERTa (Liu et al., 2019) or SpanBERT (Joshi et al., 2020) . In Chapter 9 we explored causal (left-to-right) transformers that can serve as the basis for powerful language models-models that can easily be applied to autoregressive generation problems such as contextual generation, summarization and machine translation. However, when applied to sequence classification and labeling problems causal models have obvious shortcomings since they are based on an incremental, left-to-right processing of their inputs. If we want to assign the correct named-entity tag to each word in a sentence, or other sophisticated linguistic labels like the parse tags we'll introduce in later chapters, we'll want to be able to take into account information from the right context as we process each element. Figure 11 .1, reproduced here from Chapter 9, illustrates the information flow in the purely left-to-right approach of Chapter 9. As can be seen, the hidden state computation at each point in time is based solely on the current and earlier elements of the input, ignoring potentially useful information located to the right of each tagging decision.", "Figure 11 .1 A causal, backward looking, transformer model like Chapter 9. Each output is computed independently of the others using only information seen earlier in the context.", "Figure 11 .2 Information flow in a bidirectional self-attention model. In processing each element of the sequence, the model attends to all inputs, both before and after the current one.", "Bidirectional encoders overcome this limitation by allowing the self-attention mechanism to range over the entire input, as shown in Figure 11 .2. The focus of bidirectional encoders is on computing contextualized representations of the tokens in an input sequence that are generally useful across a range of downstream applications. Therefore, bidirectional encoders use self-attention to map sequences of input embeddings (x 1 , ..., x n ) to sequences of output embeddings the same length (y 1 , ..., y n ), where the output vectors have been contextualized using information from the entire input sequence.", "Since each output vector, y i , is computed independently, the processing of an entire sequence can be parallelized via matrix operations. The first step is to pack the input embeddings x i into a matrix X \u2208 R N\u00d7d h . That is, each row of X is the embedding of one token of the input. We then multiply X by the key, query, and value weight matrices (all of dimensionality d \u00d7 d) to produce matrices Q \u2208 R N\u00d7d , K \u2208 R N\u00d7d , and V \u2208 R N\u00d7d , containing all the key, query, and value vectors in a single step.", "Given these matrices we can compute all the requisite query-key comparisons simultaneously by multiplying Q and K in a single operation. Figure 11 .3 illustrates the result of this operation for an input with length 5.", "Finally, we can scale these scores, take the softmax, and then multiply the result by V resulting in a matrix of shape N \u00d7 d where each row contains a contextualized output embedding corresponding to each token in the input.", "More specifically, the original input sequence is first tokenized using a subword model. The sampled items which drive the learning process are chosen from among the set of tokenized inputs. Word embeddings for all of the tokens in the input are retrieved from the word embedding matrix and then combined with positional embeddings to form the input to the transformer.", "Figure 11 .5 Masked language model training. In this example, three of the input tokens are selected, two of which are masked and the third is replaced with an unrelated word. The probabilities assigned by the model to these three items are used as the training loss. (In this and subsequent figures we display the input as words rather than subword tokens; the reader should keep in mind that BERT and similar models actually use subword tokens instead.)", "Figure 11 .5 illustrates this approach with a simple example. Here, long, thanks and the have been sampled from the training sequence, with the first two masked and the replaced with the randomly sampled token apricot. The resulting embeddings are passed through a stack of bidirectional transformer blocks. To produce a probability distribution over the vocabulary for each of the masked tokens, the output vector from the final transformer layer for each of the masked tokens is multiplied by a learned set of classification weights W V \u2208 R |V |\u00d7d h and then through a softmax to yield the required predictions over the vocabulary.", "With a predicted probability distribution for each masked item, we can use crossentropy to compute the loss for each masked item-the negative log probability assigned to the actual masked word, as shown in Figure 11 .5. The gradients that form the basis for the weight updates are based on the average loss over the sampled learning items from a single training sequence (or batch of sequences).", "The final loss is the sum of the BERT MLM loss and the SBO loss. Figure 11 .6 illustrates this with one of our earlier examples. Here the span selected is and thanks for which spans from position 3 to 5. The total loss associated with the masked token thanks is the sum of the cross-entropy loss generated from the prediction of thanks from the output y 4 , plus the cross-entropy loss from the prediction of thanks from the output vectors for y 2 , y 6 and the embedding for position 4 in the span. Figure 11 .6 Span-based language model training. In this example, a span of length 3 is selected for training and all of the words in the span are masked. The figure illustrates the loss computed for word thanks; the loss for the entire span is based on the loss for all three of the words in the span.", "Cross entropy is used to compute the NSP loss for each sentence pair presented to the model. Figure 11 .7 illustrates the overall NSP training setup. In BERT, the NSP loss was used in conjunction with the MLM training objective to form final loss. Figure 11 .7 An example of the NSP loss calculation.", "A key difference from what we've seen earlier with neural classifiers is that this loss can be used to not only learn the weights of the classifier, but also to update the weights for the pretrained language model itself. In practice, reasonable classification performance is typically achieved with only minimal changes to the language model parameters, often limited to updates over the final few layers of the transformer. Figure 11 .8 illustrates this overall approach to sequence classification.", "Sequence labelling tasks, such as part-of-speech tagging or BIO-based named entity recognition, follow the same basic classification approach. Here, the final output vector corresponding to each input token is passed to a classifier that produces a softmax distribution over the possible set of tags. Again, assuming a simple classifier consisting of a single feedforward layer followed by a softmax, the set of weights to be learned for this additional layer is W K \u2208 R k\u00d7d h , where k is the number of possible tags for the task. As with RNNs, a greedy approach, where the argmax tag for each token is taken as a likely answer, can be used to generate the final output tag sequence. Figure 11 .9 illustrates an example of this approach.", "Formally, given an input sequence x consisting of T tokens, (x 1 , x 2 , ..., x T ), a span is a contiguous sequence of tokens with start i and end j such that 1 <= i <= j <= T . This formulation results in a total set of spans equal to T (T \u22121)", ". For practical purposes, span-based models often impose an application-specific length limit L, so the legal spans are limited to those where j \u2212 i < L. In the following, we'll refer to the enumerated set of legal spans in x as S(x).", "Now, given span representations g for each span in S(x), classifiers can be finetuned to generate application-specific scores for various span-oriented tasks: binary span identification (is this a legitimate span of interest or not? ), span classification (what kind of span is this? ), and span relation classification (how are these two spans related? ).", "To ground this discussion, let's return to named entity recognition (NER). Given a scheme for representing spans and set of named entity types, a span-based approach to NER is a straightforward classification problem where each span in an input is assigned a class label. More formally, given an input sequence x, we want to assign a label y, from the set of valid NER labels, to each of the spans in S(x). Since most of the spans in a given input will not be named entities we'll add the label NULL to the set of types in Y . Figure 11 .10 A span-oriented approach to named entity classification. The figure only illustrates the computation for 2 spans corresponding to ground truth named entities. In reality, the network scores all of the", "During decoding, each span is scored using a softmax over the final classifier output to generate a distribution over the possible labels, with the argmax score for each span taken as the correct answer. Figure 11 .10 illustrates this approach with an example. A variation on this scheme designed to improve precision adds a calibrated threshold to the labeling of a span as anything other than NULL.", "Finally, there are important privacy issues. Language models, like other machine learning models, can leak information about their training data. It is thus possible for an adversary to extract individual training-data phrases from a language model such as an individual person's name, phone number, and address (Henderson et al. 2017 , Carlini et al. 2020 . This is a problem if large language models are trained on private datasets such has electronic health records (EHRs). Mitigating all these harms is an important but unsolved research question in NLP. Extra pretraining (Gururangan et al., 2020) on non-toxic subcorpora seems to reduce a language model's tendency to generate toxic language somewhat (Gehman et al., 2020) . And analyzing the data used to pretrain large language models is important to understand toxicity and bias in generation, as well as privacy, making it extremely important that language models include datasheets (page ??) or model cards (page ??) giving full replicable information on the corpora used to train them."]